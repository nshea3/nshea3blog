[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/2020-12-02-mathematics-of-drilling-intercepts.html",
    "href": "posts/2020-12-02-mathematics-of-drilling-intercepts.html",
    "title": "The Mathematics of Drilling Intercepts",
    "section": "",
    "text": "Drilling intercepts are a prominent feature of junior mining news releases. They are closely monitored by the mining investment community, and a particularly good intercept can raise the prospects for a project.\nAs an example, consider this November 10 2020 release from Freegold Ventures:\n\nFreegold Intercepts 3.78 g/t Au Over 119 Metres Including 131.5 g/t Over 3 Metres Within 573 Metres of 1.21 g/t Au at Golden Summit\n\nThe market responded with a 3% boost in the share price the next trading day, so clearly this was regarded as a positive signal for the company’s prospects. (This is typical: capital markets tend to treat any news of this sort as good news.)\nThe implications for the economic, geological, and engineering variables surrounding the project are much less clear. Is this a good geological result? Is it a good engineering result? Intercepts are highlights: incomplete data, collected and released selectively, so is it even possible to make an informed judgement using these numbers?\nTo complicate things even further, the selectively reported drilling intercepts are usually released in a rather complex manner, which can make it difficult to distinguish between truly good numbers and deceptively good results. Drilling intercepts are discussed at great length in other sources (here and here and here) but we’ll take a mathematical perspective and develop a model that describes nested intercept configurations of arbitrary complexity.\nWe’ll take Great Bear Resources for an extended example. Great Bear Resources is a Canadian junior mining company whose stock gained substantially on announcement of very high grade intercepts at their Dixie project in Ontario. At time of writing, GBR is trading at a $886 million CAD market cap (which is not very bearish at all!)\n\n\n\n\n\n\nGBR Price Today by TradingView\n\n\n\n\nHere we open up the spreadsheet of drilling results (available on their website), and then filter on Drill Hole to consider a single hole:\n\nimport pandas as pd\n\nintercepts = pd.read_excel('drive/My Drive/Projects/posts/data/Great_Bear/lp_drill_hole_composites_all.xlsx')\nintercepts['Record'] = intercepts.index\n\ndh = intercepts[intercepts['Drill Hole'] == 'BR-022']\n\ndh\n\n\n\n\n\n\n\n\nDrill Hole\nUnnamed: 1\nFrom (m)\nTo (m)\nWidth (m)\nGold (g/t)\nRecord\n\n\n\n\n95\nBR-022\nincluding\n110.0\n116.10\n6.10\n2.62\n95\n\n\n96\nBR-022\nand including\n111.4\n113.10\n1.70\n7.92\n96\n\n\n97\nBR-022\nand\n274.0\n299.00\n25.00\n0.19\n97\n\n\n98\nBR-022\nand\n432.9\n439.00\n6.10\n4.05\n98\n\n\n99\nBR-022\nincluding\n432.9\n435.70\n2.80\n8.18\n99\n\n\n100\nBR-022\nand\n445.0\n452.00\n7.00\n0.41\n100\n\n\n101\nBR-022\nand\n461.6\n512.00\n50.40\n1.78\n101\n\n\n102\nBR-022\nincluding\n471.0\n512.00\n41.00\n2.09\n102\n\n\n103\nBR-022\nand including\n471.0\n478.00\n7.00\n2.37\n103\n\n\n104\nBR-022\nand including\n490.0\n491.00\n1.00\n8.15\n104\n\n\n105\nBR-022\nand including\n505.2\n508.75\n3.55\n14.90\n105\n\n\n106\nBR-022\nand including\n506.2\n506.70\n0.50\n100.48\n106\n\n\n107\nBR-022\nand\n600.0\n620.00\n20.00\n0.52\n107\n\n\n\n\n\n\n\nThis is how intercepts are typically presented: a table with a From field describing where they started measuring, a To field describing where they stopped, and a Grade field (called Gold here) that tells us how enriched that interval is with the valuable stuff. From and To are typically measured downhole from the drill collar.\nIt’s easy to establish a basic understanding of how these tables are read, and many experienced mining investors immediately recognize these grades as very high. The rest of us might need to rely on statistics, since we don’t have the benefit of many years’ experience with drilling results.\nOf course it is first necessary to determine the true assay values for each separate interval from top to bottom. Unfortunately, each row is not independent - some of the intercepts are contained in others, and the subinterval gold is INCLUDED in the parent interval calculation! So we can’t just use the Gold (g/t) field directly, since intercepts are reported with these “highlights”, or higher grade sections within the longer interval.\nSometimes this convention is used unethically to suggest larger intervals of enrichment than truly exist. This is called “grade smearing” and the method of residual grade calculation applied here will detect any such attempt to disguise poor results.\nAt first it may seem like the correct interpretation of these intervals is to imagine them intervals stacked on top of one another, but this is very misleading. We can easily visualize this to see the error:\n\n\n\n\n\n\n\n\nWe only have the total grade, INCLUDING the high-grade, child subintervals. Considering it in that way ignores the fact that the high-grade intervals are included in the wider, lower-grade intervals, inflating the grade measured over that length. This has enormous implications for the continuity of the mineralization, which determines the feasibility of the project.\n\nIn order to eliminate this effect we’ll need to do some math with the intercepts. This visualization attempts to show this hierarchical, branching structure:\n\n\n\n\n\n\n\nPlotted side by side, the intercepts show the parent-child overlapping relationship and capture the complexity of the problem.\nParent intervals can have no child intervals, a single child interval, or several child intervals. Child intervals themselves can have no child intervals, a single child interval, or several child intervals. Clearly there is a whole class of related problems we could solve with a general solution to this problem.\nSo far we have treated the From and To fields in isolation, and we can use a cool feature of Pandas to convert them to intervals:\n\ndh['Interval'] = dh.apply(lambda x: pd.Interval(x['From (m)'], x['To (m)']), axis=1)\n\ndh\n\n/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"Entry point for launching an IPython kernel.\n\n\nSo the motivation here was to create Interval objects to use them with the pd.Overlaps function and then model the overlap relationship among the different intervals:\n\nimport itertools\nimport numpy as np\n\ncross_interval = itertools.product(dh.Interval,dh.Interval)\n\n\n\noverlap_matrix = np.array([interval[0].overlaps(interval[1]) for interval in cross_interval])\n\nintersect_matrix = np.array([interval[0].intersect(interval[1]) for interval in cross_interval])\n\nns = int(np.sqrt(overlap_matrix.shape[0]))\n\n\n\n\n\n\n\n\n\n\nHere we see the overlaps: if a pixel is white, it means that the interval on the x-axis and the interval on the y-axis overlap.\nOverlap is symmetric: so each ‘child’ overlaps with its parent and vice versa. It should become clear that we are actually interested in the “contains” relationship, which is not symmetric and will help us identify parent intervals and child intervals and start reducing the intervals.\nFortunately this is also supported in Python:\n\n\n\n\n\n\n\n\n\nNow we can pull out a tree\nOf the machine-intelligible formats, a tree data structure is clearly the most suited to representing the intervals.\n\nfor i, col in enumerate(contain_matrix_sq.T):\n\n  if ~np.any(col):\n\n    all_str_to_node[str(dh['Record'].values[i])].parent = root\n\n  else:\n\n    all_str_to_node[str(dh['Record'].values[i])].parent = all_str_to_node[str(dh['Record'].values[::-1][np.argmax(col[::-1])])]\n\nprint(RenderTree(root, style=AsciiStyle()).by_attr())\n\nDH\n|-- 95\n|   +-- 96\n|-- 97\n|-- 98\n|   +-- 99\n|-- 100\n|-- 101\n|   +-- 102\n|       |-- 103\n|       |-- 104\n|       +-- 105\n|           +-- 106\n+-- 107\n\n\nNow we are really getting somewhere- we can actually start looking at the global picture (since we now know which intervals are not “child” intervals)\nThese are the direct children. We can go ahead and plot them and have a totally accurate picture of the log:\n\n\n\n\n\n\n\n\ndh_prime.dtypes\n\nDrill Hole                 object\nUnnamed: 1                 object\nFrom (m)                  float64\nTo (m)                    float64\nWidth (m)                 float64\nGold (g/t)                float64\nRecord                      int64\nInterval        interval[float64]\nInterval_obj               object\ndtype: object\n\n\nWhile that is correct, it is not complete: we have left out all of the additional information provided by the smaller sub-intervals!\nIn order to incorporate that we will have to remove them from the parent intervals and determine the residual grade (whatever is left once we pull out the gold contained in the subinterval)\n\n((119) * (3.78) - (3) * (131.5)) / (119 - 3)\n\n0.47689655172413786\n\n\nAs an example of this kind of calculation, a simpler set of intervals from a Freegold Ventures press release:\n\nFreegold Intercepts 3.78 g/t Au Over 119 Metres Including 131.5 g/t Over 3 Metres Within 573 Metres of 1.21 g/t Au at Golden Summit\n\nWe know the gold grade over the whole 119 meters, and the gold grade over 3 meters, but what is the gold grade over the \\(119 - 3 = 116 m\\)?\nThe solution is a simple weighted average calculation, like compositing over a drillhole:\n\\(\\frac{119 \\times 3.78-3 \\times 131.5}{119-3} = 0.477 g/t\\)\nCredit to https://twitter.com/BrentCo77759016/status/1326183861722599424 and\nSo now we have to do this, but with every single subinterval until we get the residual grade at every point along the drillhole\nFortunately, the tree data structure we selected has specialized methods that make a traversal very simple.\n\nlevelord_nodes = [(node.name, node.children) for node in LevelOrderIter(root)]\n\nlevelord_nodes\n\n[('DH',\n  (Node('/DH/95'),\n   Node('/DH/97'),\n   Node('/DH/98'),\n   Node('/DH/100'),\n   Node('/DH/101'),\n   Node('/DH/107'))),\n ('95', (Node('/DH/95/96'),)),\n ('97', ()),\n ('98', (Node('/DH/98/99'),)),\n ('100', ()),\n ('101', (Node('/DH/101/102'),)),\n ('107', ()),\n ('96', ()),\n ('99', ()),\n ('102',\n  (Node('/DH/101/102/103'), Node('/DH/101/102/104'), Node('/DH/101/102/105'))),\n ('103', ()),\n ('104', ()),\n ('105', (Node('/DH/101/102/105/106'),)),\n ('106', ())]\n\n\n\nnn_np_loi = [(node.name, node.parent) for node in LevelOrderIter(root)]\n\n\nall_str_to_node\n\n{'100': Node('/DH/100'),\n '101': Node('/DH/101'),\n '102': Node('/DH/101/102'),\n '103': Node('/DH/101/102/103'),\n '104': Node('/DH/101/102/104'),\n '105': Node('/DH/101/102/105'),\n '106': Node('/DH/101/102/105/106'),\n '107': Node('/DH/107'),\n '95': Node('/DH/95'),\n '96': Node('/DH/95/96'),\n '97': Node('/DH/97'),\n '98': Node('/DH/98'),\n '99': Node('/DH/98/99')}\n\n\n\nfor node, parent in nn_np_loi[::-1][:-1]:\n  print(node)\n  for child in all_str_to_node[node].children:\n    print(child)\n\n106\n105\nNode('/DH/101/102/105/106')\n104\n103\n102\nNode('/DH/101/102/103')\nNode('/DH/101/102/104')\nNode('/DH/101/102/105')\n99\n96\n107\n101\nNode('/DH/101/102')\n100\n98\nNode('/DH/98/99')\n97\n95\nNode('/DH/95/96')\n\n\n\ndh\n\n\n\n\n\n\n\n\nDrill Hole\nUnnamed: 1\nFrom (m)\nTo (m)\nWidth (m)\nGold (g/t)\nRecord\nInterval\nInterval_obj\n\n\n\n\n95\nBR-022\nincluding\n110.0\n116.10\n6.10\n2.62\n95\n(110.0, 116.1]\nInterval(110.000000000000, 116.100000000000)\n\n\n96\nBR-022\nand including\n111.4\n113.10\n1.70\n7.92\n96\n(111.4, 113.1]\nInterval(111.400000000000, 113.100000000000)\n\n\n97\nBR-022\nand\n274.0\n299.00\n25.00\n0.19\n97\n(274.0, 299.0]\nInterval(274.000000000000, 299.000000000000)\n\n\n98\nBR-022\nand\n432.9\n439.00\n6.10\n4.05\n98\n(432.9, 439.0]\nInterval(432.900000000000, 439.000000000000)\n\n\n99\nBR-022\nincluding\n432.9\n435.70\n2.80\n8.18\n99\n(432.9, 435.7]\nInterval(432.900000000000, 435.700000000000)\n\n\n100\nBR-022\nand\n445.0\n452.00\n7.00\n0.41\n100\n(445.0, 452.0]\nInterval(445.000000000000, 452.000000000000)\n\n\n101\nBR-022\nand\n461.6\n512.00\n50.40\n1.78\n101\n(461.6, 512.0]\nInterval(461.600000000000, 512.000000000000)\n\n\n102\nBR-022\nincluding\n471.0\n512.00\n41.00\n2.09\n102\n(471.0, 512.0]\nInterval(471.000000000000, 512.000000000000)\n\n\n103\nBR-022\nand including\n471.0\n478.00\n7.00\n2.37\n103\n(471.0, 478.0]\nInterval(471.000000000000, 478.000000000000)\n\n\n104\nBR-022\nand including\n490.0\n491.00\n1.00\n8.15\n104\n(490.0, 491.0]\nInterval(490.000000000000, 491.000000000000)\n\n\n105\nBR-022\nand including\n505.2\n508.75\n3.55\n14.90\n105\n(505.2, 508.75]\nInterval(505.200000000000, 508.750000000000)\n\n\n106\nBR-022\nand including\n506.2\n506.70\n0.50\n100.48\n106\n(506.2, 506.7]\nInterval(506.200000000000, 506.700000000000)\n\n\n107\nBR-022\nand\n600.0\n620.00\n20.00\n0.52\n107\n(600.0, 620.0]\nInterval(600.000000000000, 620.000000000000)\n\n\n\n\n\n\n\n\ncross_interval = itertools.product(dh.Interval_obj,dh.Interval_obj)\n\nintersect_matrix = np.array([interval[0].intersect(interval[1]) for interval in cross_interval])\n\nintersect_matrix\n\narray([Interval(110.000000000000, 116.100000000000),\n       Interval(111.400000000000, 113.100000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       Interval(111.400000000000, 113.100000000000),\n       Interval(111.400000000000, 113.100000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(),\n       Interval(274.000000000000, 299.000000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(),\n       Interval(432.900000000000, 439.000000000000),\n       Interval(432.900000000000, 435.700000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       Interval(432.900000000000, 435.700000000000),\n       Interval(432.900000000000, 435.700000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(),\n       Interval(445.000000000000, 452.000000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(),\n       Interval(461.600000000000, 512.000000000000),\n       Interval(471.000000000000, 512.000000000000),\n       Interval(471.000000000000, 478.000000000000),\n       Interval(490.000000000000, 491.000000000000),\n       Interval(505.200000000000, 508.750000000000),\n       Interval(506.200000000000, 506.700000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), Interval(471.000000000000, 512.000000000000),\n       Interval(471.000000000000, 512.000000000000),\n       Interval(471.000000000000, 478.000000000000),\n       Interval(490.000000000000, 491.000000000000),\n       Interval(505.200000000000, 508.750000000000),\n       Interval(506.200000000000, 506.700000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), Interval(471.000000000000, 478.000000000000),\n       Interval(471.000000000000, 478.000000000000),\n       Interval(471.000000000000, 478.000000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       Interval(490.000000000000, 491.000000000000),\n       Interval(490.000000000000, 491.000000000000), EmptySet(),\n       Interval(490.000000000000, 491.000000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(),\n       Interval(505.200000000000, 508.750000000000),\n       Interval(505.200000000000, 508.750000000000), EmptySet(),\n       EmptySet(), Interval(505.200000000000, 508.750000000000),\n       Interval(506.200000000000, 506.700000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), Interval(506.200000000000, 506.700000000000),\n       Interval(506.200000000000, 506.700000000000), EmptySet(),\n       EmptySet(), Interval(506.200000000000, 506.700000000000),\n       Interval(506.200000000000, 506.700000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(),\n       Interval(600.000000000000, 620.000000000000)], dtype=object)\n\n\n\ndh['grade_len'] = dh['Gold (g/t)'] * dh['Width (m)']\n\n/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"Entry point for launching an IPython kernel.\n\n\n\ndh\n\n\n\n\n\n\n\n\nDrill Hole\nUnnamed: 1\nFrom (m)\nTo (m)\nWidth (m)\nGold (g/t)\nRecord\nInterval\nInterval_obj\ngrade_len\n\n\n\n\n95\nBR-022\nincluding\n110.0\n116.10\n6.10\n2.62\n95\n(110.0, 116.1]\nInterval(110.000000000000, 116.100000000000)\n15.982\n\n\n96\nBR-022\nand including\n111.4\n113.10\n1.70\n7.92\n96\n(111.4, 113.1]\nInterval(111.400000000000, 113.100000000000)\n13.464\n\n\n97\nBR-022\nand\n274.0\n299.00\n25.00\n0.19\n97\n(274.0, 299.0]\nInterval(274.000000000000, 299.000000000000)\n4.750\n\n\n98\nBR-022\nand\n432.9\n439.00\n6.10\n4.05\n98\n(432.9, 439.0]\nInterval(432.900000000000, 439.000000000000)\n24.705\n\n\n99\nBR-022\nincluding\n432.9\n435.70\n2.80\n8.18\n99\n(432.9, 435.7]\nInterval(432.900000000000, 435.700000000000)\n22.904\n\n\n100\nBR-022\nand\n445.0\n452.00\n7.00\n0.41\n100\n(445.0, 452.0]\nInterval(445.000000000000, 452.000000000000)\n2.870\n\n\n101\nBR-022\nand\n461.6\n512.00\n50.40\n1.78\n101\n(461.6, 512.0]\nInterval(461.600000000000, 512.000000000000)\n89.712\n\n\n102\nBR-022\nincluding\n471.0\n512.00\n41.00\n2.09\n102\n(471.0, 512.0]\nInterval(471.000000000000, 512.000000000000)\n85.690\n\n\n103\nBR-022\nand including\n471.0\n478.00\n7.00\n2.37\n103\n(471.0, 478.0]\nInterval(471.000000000000, 478.000000000000)\n16.590\n\n\n104\nBR-022\nand including\n490.0\n491.00\n1.00\n8.15\n104\n(490.0, 491.0]\nInterval(490.000000000000, 491.000000000000)\n8.150\n\n\n105\nBR-022\nand including\n505.2\n508.75\n3.55\n14.90\n105\n(505.2, 508.75]\nInterval(505.200000000000, 508.750000000000)\n52.895\n\n\n106\nBR-022\nand including\n506.2\n506.70\n0.50\n100.48\n106\n(506.2, 506.7]\nInterval(506.200000000000, 506.700000000000)\n50.240\n\n\n107\nBR-022\nand\n600.0\n620.00\n20.00\n0.52\n107\n(600.0, 620.0]\nInterval(600.000000000000, 620.000000000000)\n10.400\n\n\n\n\n\n\n\n\nfrom sympy import Union\nimport functools\n\nresid_grades, resid_len = {}, {} \n\nfor node in levelord_nodes[1:]:\n\n  parent_interval = dh[dh['Record'] == float(node[0])]\n  child_names = [child.name for child in node[1]]\n  child_intervals = [dh[dh['Record'] == float(child)] for child in child_names]\n\n  new_interval_obj = parent_interval.Interval_obj.values[0] - Union([child.Interval_obj.values[0] for child in child_intervals])\n\n  l_child_int = Union([intv['Interval_obj'].values[0] for intv in child_intervals])._measure\n\n  lg_child_int = [dh.loc[int(child_name)]['grade_len'] for child_name in child_names]\n\n  lg_total_int = parent_interval.grade_len.values[0]\n\n  residual_grade = (lg_total_int - sum(lg_child_int)) / (new_interval_obj._measure)\n\n  resid_grades[node[0]] = residual_grade\n  resid_len[node[0]] = new_interval_obj\n\n  print(\"Interval:\")\n  print(node[0])\n\n  print(\"Length x Grade:\")\n  print(lg_total_int - sum(lg_child_int))\n\n  print(\"Residual Grade:\")\n  print(residual_grade)\n\nInterval:\n95\nLength x Grade:\n2.517999999999999\nResidual Grade:\n0.572272727272726\nInterval:\n97\nLength x Grade:\n4.75\nResidual Grade:\n0.190000000000000\nInterval:\n98\nLength x Grade:\n1.801000000000002\nResidual Grade:\n0.545757575757574\nInterval:\n100\nLength x Grade:\n2.8699999999999997\nResidual Grade:\n0.410000000000000\nInterval:\n101\nLength x Grade:\n4.022000000000006\nResidual Grade:\n0.427872340425534\nInterval:\n107\nLength x Grade:\n10.4\nResidual Grade:\n0.520000000000000\nInterval:\n96\nLength x Grade:\n13.464\nResidual Grade:\n7.92000000000005\nInterval:\n99\nLength x Grade:\n22.903999999999996\nResidual Grade:\n8.17999999999997\nInterval:\n102\nLength x Grade:\n8.055000000000007\nResidual Grade:\n0.273514431239389\nInterval:\n103\nLength x Grade:\n16.59\nResidual Grade:\n2.37000000000000\nInterval:\n104\nLength x Grade:\n8.15\nResidual Grade:\n8.15000000000000\nInterval:\n105\nLength x Grade:\n2.654999999999994\nResidual Grade:\n0.870491803278683\nInterval:\n106\nLength x Grade:\n50.24\nResidual Grade:\n100.480000000000\n\n\nCheck these solutions: 95 should be easy to validate\nincludes 96\n95 extends from 110.00 m to 116.10 m (l = 6.10 m)\n96 is the interval from 111.40 to 113.10 m (l = 1.70 m)\nLarger interval\n(6.10 m)(2.62 g/t) = (1.7 m)(7.92 g/t) + (4.4 m)(x g/t)\nRearranging terms, we get:\n\\(x = \\frac{(6.10 m)(2.62 g/t) - (1.7 m)(7.92 g/t)}{ 4.4 m }\\)\nSo the residual grade is 0.5723 g/t, which matches the value found above!\n\n((6.10 * 2.62) - (1.7)*(7.92)) / (4.4)\n\n0.5722727272727269\n\n\n\nresid_len\n\n{'100': Interval(445.000000000000, 452.000000000000),\n '101': Interval.Ropen(461.600000000000, 471.000000000000),\n '102': Union(Interval.open(478.000000000000, 490.000000000000), Interval.open(491.000000000000, 505.200000000000), Interval.Lopen(508.750000000000, 512.000000000000)),\n '103': Interval(471.000000000000, 478.000000000000),\n '104': Interval(490.000000000000, 491.000000000000),\n '105': Union(Interval.Ropen(505.200000000000, 506.200000000000), Interval.Lopen(506.700000000000, 508.750000000000)),\n '106': Interval(506.200000000000, 506.700000000000),\n '107': Interval(600.000000000000, 620.000000000000),\n '95': Union(Interval.Ropen(110.000000000000, 111.400000000000), Interval.Lopen(113.100000000000, 116.100000000000)),\n '96': Interval(111.400000000000, 113.100000000000),\n '97': Interval(274.000000000000, 299.000000000000),\n '98': Interval.Lopen(435.700000000000, 439.000000000000),\n '99': Interval(432.900000000000, 435.700000000000)}\n\n\n\nInterval(445.000000000000, 452.000000000000).end\n\n452.000000000000\n\n\n\ndh['Record'].astype(str).map(resid_len)\ndh['Record'].astype(str).map(resid_grades)\n\n95     0.572272727272726\n96      7.92000000000005\n97     0.190000000000000\n98     0.545757575757574\n99      8.17999999999997\n100    0.410000000000000\n101    0.427872340425534\n102    0.273514431239389\n103     2.37000000000000\n104     8.15000000000000\n105    0.870491803278683\n106     100.480000000000\n107    0.520000000000000\nName: Record, dtype: object\n\n\nTODO: Need to split up the non-contiguous segments so that you can actually plot them\nIdea: use .args attribute Problem: This is defined for Interval objects as well and it gets us something we don’t want\n\ndh['Record'].astype(str).map(resid_len)\n\n95     Union(Interval.Ropen(110.000000000000, 111.400...\n96          Interval(111.400000000000, 113.100000000000)\n97          Interval(274.000000000000, 299.000000000000)\n98     Interval.Lopen(435.700000000000, 439.000000000...\n99          Interval(432.900000000000, 435.700000000000)\n100         Interval(445.000000000000, 452.000000000000)\n101    Interval.Ropen(461.600000000000, 471.000000000...\n102    Union(Interval.open(478.000000000000, 490.0000...\n103         Interval(471.000000000000, 478.000000000000)\n104         Interval(490.000000000000, 491.000000000000)\n105    Union(Interval.Ropen(505.200000000000, 506.200...\n106         Interval(506.200000000000, 506.700000000000)\n107         Interval(600.000000000000, 620.000000000000)\nName: Record, dtype: object\n\n\n\n\n\ndef args_extended(interval):\n  if type(interval) == Union:\n    return interval.args\n  else:\n    return interval\n\nremaining_interval_grade = pd.DataFrame({'interval' : dh['Record'].astype(str).map(resid_len), 'grade' : dh['Record'].astype(str).map(resid_grades)})\n\nremaining_interval_grade['split_intervals'] = remaining_interval_grade.interval.apply(args_extended)\n\nrig_exploded = remaining_interval_grade.explode('split_intervals')\n\n\nrig_exploded\n\n\n\n\n\n\n\n\ninterval\ngrade\nsplit_intervals\n\n\n\n\n95\nUnion(Interval.Ropen(110.000000000000, 111.400...\n0.572272727272726\nInterval.Ropen(110.000000000000, 111.400000000...\n\n\n95\nUnion(Interval.Ropen(110.000000000000, 111.400...\n0.572272727272726\nInterval.Lopen(113.100000000000, 116.100000000...\n\n\n96\nInterval(111.400000000000, 113.100000000000)\n7.92000000000005\nInterval(111.400000000000, 113.100000000000)\n\n\n97\nInterval(274.000000000000, 299.000000000000)\n0.190000000000000\nInterval(274.000000000000, 299.000000000000)\n\n\n98\nInterval.Lopen(435.700000000000, 439.000000000...\n0.545757575757574\nInterval.Lopen(435.700000000000, 439.000000000...\n\n\n99\nInterval(432.900000000000, 435.700000000000)\n8.17999999999997\nInterval(432.900000000000, 435.700000000000)\n\n\n100\nInterval(445.000000000000, 452.000000000000)\n0.410000000000000\nInterval(445.000000000000, 452.000000000000)\n\n\n101\nInterval.Ropen(461.600000000000, 471.000000000...\n0.427872340425534\nInterval.Ropen(461.600000000000, 471.000000000...\n\n\n102\nUnion(Interval.open(478.000000000000, 490.0000...\n0.273514431239389\nInterval.open(478.000000000000, 490.000000000000)\n\n\n102\nUnion(Interval.open(478.000000000000, 490.0000...\n0.273514431239389\nInterval.open(491.000000000000, 505.200000000000)\n\n\n102\nUnion(Interval.open(478.000000000000, 490.0000...\n0.273514431239389\nInterval.Lopen(508.750000000000, 512.000000000...\n\n\n103\nInterval(471.000000000000, 478.000000000000)\n2.37000000000000\nInterval(471.000000000000, 478.000000000000)\n\n\n104\nInterval(490.000000000000, 491.000000000000)\n8.15000000000000\nInterval(490.000000000000, 491.000000000000)\n\n\n105\nUnion(Interval.Ropen(505.200000000000, 506.200...\n0.870491803278683\nInterval.Ropen(505.200000000000, 506.200000000...\n\n\n105\nUnion(Interval.Ropen(505.200000000000, 506.200...\n0.870491803278683\nInterval.Lopen(506.700000000000, 508.750000000...\n\n\n106\nInterval(506.200000000000, 506.700000000000)\n100.480000000000\nInterval(506.200000000000, 506.700000000000)\n\n\n107\nInterval(600.000000000000, 620.000000000000)\n0.520000000000000\nInterval(600.000000000000, 620.000000000000)\n\n\n\n\n\n\n\n\nrig_exploded['From'] = rig_exploded.split_intervals.apply(lambda x: x.start).astype(float)\nrig_exploded['To'] = rig_exploded.split_intervals.apply(lambda x: x.end).astype(float)\nrig_exploded['Width'] = (rig_exploded.To - rig_exploded.From).astype(float)\n\nrig_exploded['grade'] = rig_exploded.grade.astype(float)\n\nrig_exploded['drillhole'] = 'BR-022'    \n\n\nrig_exploded[['From', 'To', 'grade', 'Width']]\n\n\n\n\n\n\n\n\nFrom\nTo\ngrade\nWidth\n\n\n\n\n95\n110.00\n111.40\n0.572273\n1.40\n\n\n95\n113.10\n116.10\n0.572273\n3.00\n\n\n96\n111.40\n113.10\n7.920000\n1.70\n\n\n97\n274.00\n299.00\n0.190000\n25.00\n\n\n98\n435.70\n439.00\n0.545758\n3.30\n\n\n99\n432.90\n435.70\n8.180000\n2.80\n\n\n100\n445.00\n452.00\n0.410000\n7.00\n\n\n101\n461.60\n471.00\n0.427872\n9.40\n\n\n102\n478.00\n490.00\n0.273514\n12.00\n\n\n102\n491.00\n505.20\n0.273514\n14.20\n\n\n102\n508.75\n512.00\n0.273514\n3.25\n\n\n103\n471.00\n478.00\n2.370000\n7.00\n\n\n104\n490.00\n491.00\n8.150000\n1.00\n\n\n105\n505.20\n506.20\n0.870492\n1.00\n\n\n105\n506.70\n508.75\n0.870492\n2.05\n\n\n106\n506.20\n506.70\n100.480000\n0.50\n\n\n107\n600.00\n620.00\n0.520000\n20.00\n\n\n\n\n\n\n\n\n\ny_axis = alt.Axis(\n    title='Intercept ID',\n    offset=5,\n    ticks=False,\n    domain=False\n)\n\n\nreqd_cols = ['From', 'To', 'grade', 'Width', 'drillhole']\n\n\nalt.Chart(rig_exploded[reqd_cols]).mark_bar().encode(\n    alt.X('From:Q',\n        scale=alt.Scale(zero=False)),\n    x2='To:Q',\n    y=alt.Y('drillhole:N', axis=y_axis),\n    color=alt.Color('grade:Q', scale=alt.Scale(scheme=\"inferno\")),\n    tooltip=[\n        alt.Tooltip('Width:Q', title='Width'),\n        alt.Tooltip('grade:Q', title='Gold Grade')\n    ]\n).properties(width=800, height=100).configure(background='#D9E9F0').interactive()"
  },
  {
    "objectID": "posts/2020-12-02-mathematics-of-drilling-intercepts.html#introduction",
    "href": "posts/2020-12-02-mathematics-of-drilling-intercepts.html#introduction",
    "title": "The Mathematics of Drilling Intercepts",
    "section": "",
    "text": "Drilling intercepts are a prominent feature of junior mining news releases. They are closely monitored by the mining investment community, and a particularly good intercept can raise the prospects for a project.\nAs an example, consider this November 10 2020 release from Freegold Ventures:\n\nFreegold Intercepts 3.78 g/t Au Over 119 Metres Including 131.5 g/t Over 3 Metres Within 573 Metres of 1.21 g/t Au at Golden Summit\n\nThe market responded with a 3% boost in the share price the next trading day, so clearly this was regarded as a positive signal for the company’s prospects. (This is typical: capital markets tend to treat any news of this sort as good news.)\nThe implications for the economic, geological, and engineering variables surrounding the project are much less clear. Is this a good geological result? Is it a good engineering result? Intercepts are highlights: incomplete data, collected and released selectively, so is it even possible to make an informed judgement using these numbers?\nTo complicate things even further, the selectively reported drilling intercepts are usually released in a rather complex manner, which can make it difficult to distinguish between truly good numbers and deceptively good results. Drilling intercepts are discussed at great length in other sources (here and here and here) but we’ll take a mathematical perspective and develop a model that describes nested intercept configurations of arbitrary complexity.\nWe’ll take Great Bear Resources for an extended example. Great Bear Resources is a Canadian junior mining company whose stock gained substantially on announcement of very high grade intercepts at their Dixie project in Ontario. At time of writing, GBR is trading at a $886 million CAD market cap (which is not very bearish at all!)\n\n\n\n\n\n\nGBR Price Today by TradingView\n\n\n\n\nHere we open up the spreadsheet of drilling results (available on their website), and then filter on Drill Hole to consider a single hole:\n\nimport pandas as pd\n\nintercepts = pd.read_excel('drive/My Drive/Projects/posts/data/Great_Bear/lp_drill_hole_composites_all.xlsx')\nintercepts['Record'] = intercepts.index\n\ndh = intercepts[intercepts['Drill Hole'] == 'BR-022']\n\ndh\n\n\n\n\n\n\n\n\nDrill Hole\nUnnamed: 1\nFrom (m)\nTo (m)\nWidth (m)\nGold (g/t)\nRecord\n\n\n\n\n95\nBR-022\nincluding\n110.0\n116.10\n6.10\n2.62\n95\n\n\n96\nBR-022\nand including\n111.4\n113.10\n1.70\n7.92\n96\n\n\n97\nBR-022\nand\n274.0\n299.00\n25.00\n0.19\n97\n\n\n98\nBR-022\nand\n432.9\n439.00\n6.10\n4.05\n98\n\n\n99\nBR-022\nincluding\n432.9\n435.70\n2.80\n8.18\n99\n\n\n100\nBR-022\nand\n445.0\n452.00\n7.00\n0.41\n100\n\n\n101\nBR-022\nand\n461.6\n512.00\n50.40\n1.78\n101\n\n\n102\nBR-022\nincluding\n471.0\n512.00\n41.00\n2.09\n102\n\n\n103\nBR-022\nand including\n471.0\n478.00\n7.00\n2.37\n103\n\n\n104\nBR-022\nand including\n490.0\n491.00\n1.00\n8.15\n104\n\n\n105\nBR-022\nand including\n505.2\n508.75\n3.55\n14.90\n105\n\n\n106\nBR-022\nand including\n506.2\n506.70\n0.50\n100.48\n106\n\n\n107\nBR-022\nand\n600.0\n620.00\n20.00\n0.52\n107\n\n\n\n\n\n\n\nThis is how intercepts are typically presented: a table with a From field describing where they started measuring, a To field describing where they stopped, and a Grade field (called Gold here) that tells us how enriched that interval is with the valuable stuff. From and To are typically measured downhole from the drill collar.\nIt’s easy to establish a basic understanding of how these tables are read, and many experienced mining investors immediately recognize these grades as very high. The rest of us might need to rely on statistics, since we don’t have the benefit of many years’ experience with drilling results.\nOf course it is first necessary to determine the true assay values for each separate interval from top to bottom. Unfortunately, each row is not independent - some of the intercepts are contained in others, and the subinterval gold is INCLUDED in the parent interval calculation! So we can’t just use the Gold (g/t) field directly, since intercepts are reported with these “highlights”, or higher grade sections within the longer interval.\nSometimes this convention is used unethically to suggest larger intervals of enrichment than truly exist. This is called “grade smearing” and the method of residual grade calculation applied here will detect any such attempt to disguise poor results.\nAt first it may seem like the correct interpretation of these intervals is to imagine them intervals stacked on top of one another, but this is very misleading. We can easily visualize this to see the error:\n\n\n\n\n\n\n\n\nWe only have the total grade, INCLUDING the high-grade, child subintervals. Considering it in that way ignores the fact that the high-grade intervals are included in the wider, lower-grade intervals, inflating the grade measured over that length. This has enormous implications for the continuity of the mineralization, which determines the feasibility of the project.\n\nIn order to eliminate this effect we’ll need to do some math with the intercepts. This visualization attempts to show this hierarchical, branching structure:\n\n\n\n\n\n\n\nPlotted side by side, the intercepts show the parent-child overlapping relationship and capture the complexity of the problem.\nParent intervals can have no child intervals, a single child interval, or several child intervals. Child intervals themselves can have no child intervals, a single child interval, or several child intervals. Clearly there is a whole class of related problems we could solve with a general solution to this problem.\nSo far we have treated the From and To fields in isolation, and we can use a cool feature of Pandas to convert them to intervals:\n\ndh['Interval'] = dh.apply(lambda x: pd.Interval(x['From (m)'], x['To (m)']), axis=1)\n\ndh\n\n/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"Entry point for launching an IPython kernel.\n\n\nSo the motivation here was to create Interval objects to use them with the pd.Overlaps function and then model the overlap relationship among the different intervals:\n\nimport itertools\nimport numpy as np\n\ncross_interval = itertools.product(dh.Interval,dh.Interval)\n\n\n\noverlap_matrix = np.array([interval[0].overlaps(interval[1]) for interval in cross_interval])\n\nintersect_matrix = np.array([interval[0].intersect(interval[1]) for interval in cross_interval])\n\nns = int(np.sqrt(overlap_matrix.shape[0]))\n\n\n\n\n\n\n\n\n\n\nHere we see the overlaps: if a pixel is white, it means that the interval on the x-axis and the interval on the y-axis overlap.\nOverlap is symmetric: so each ‘child’ overlaps with its parent and vice versa. It should become clear that we are actually interested in the “contains” relationship, which is not symmetric and will help us identify parent intervals and child intervals and start reducing the intervals.\nFortunately this is also supported in Python:\n\n\n\n\n\n\n\n\n\nNow we can pull out a tree\nOf the machine-intelligible formats, a tree data structure is clearly the most suited to representing the intervals.\n\nfor i, col in enumerate(contain_matrix_sq.T):\n\n  if ~np.any(col):\n\n    all_str_to_node[str(dh['Record'].values[i])].parent = root\n\n  else:\n\n    all_str_to_node[str(dh['Record'].values[i])].parent = all_str_to_node[str(dh['Record'].values[::-1][np.argmax(col[::-1])])]\n\nprint(RenderTree(root, style=AsciiStyle()).by_attr())\n\nDH\n|-- 95\n|   +-- 96\n|-- 97\n|-- 98\n|   +-- 99\n|-- 100\n|-- 101\n|   +-- 102\n|       |-- 103\n|       |-- 104\n|       +-- 105\n|           +-- 106\n+-- 107\n\n\nNow we are really getting somewhere- we can actually start looking at the global picture (since we now know which intervals are not “child” intervals)\nThese are the direct children. We can go ahead and plot them and have a totally accurate picture of the log:\n\n\n\n\n\n\n\n\ndh_prime.dtypes\n\nDrill Hole                 object\nUnnamed: 1                 object\nFrom (m)                  float64\nTo (m)                    float64\nWidth (m)                 float64\nGold (g/t)                float64\nRecord                      int64\nInterval        interval[float64]\nInterval_obj               object\ndtype: object\n\n\nWhile that is correct, it is not complete: we have left out all of the additional information provided by the smaller sub-intervals!\nIn order to incorporate that we will have to remove them from the parent intervals and determine the residual grade (whatever is left once we pull out the gold contained in the subinterval)\n\n((119) * (3.78) - (3) * (131.5)) / (119 - 3)\n\n0.47689655172413786\n\n\nAs an example of this kind of calculation, a simpler set of intervals from a Freegold Ventures press release:\n\nFreegold Intercepts 3.78 g/t Au Over 119 Metres Including 131.5 g/t Over 3 Metres Within 573 Metres of 1.21 g/t Au at Golden Summit\n\nWe know the gold grade over the whole 119 meters, and the gold grade over 3 meters, but what is the gold grade over the \\(119 - 3 = 116 m\\)?\nThe solution is a simple weighted average calculation, like compositing over a drillhole:\n\\(\\frac{119 \\times 3.78-3 \\times 131.5}{119-3} = 0.477 g/t\\)\nCredit to https://twitter.com/BrentCo77759016/status/1326183861722599424 and\nSo now we have to do this, but with every single subinterval until we get the residual grade at every point along the drillhole\nFortunately, the tree data structure we selected has specialized methods that make a traversal very simple.\n\nlevelord_nodes = [(node.name, node.children) for node in LevelOrderIter(root)]\n\nlevelord_nodes\n\n[('DH',\n  (Node('/DH/95'),\n   Node('/DH/97'),\n   Node('/DH/98'),\n   Node('/DH/100'),\n   Node('/DH/101'),\n   Node('/DH/107'))),\n ('95', (Node('/DH/95/96'),)),\n ('97', ()),\n ('98', (Node('/DH/98/99'),)),\n ('100', ()),\n ('101', (Node('/DH/101/102'),)),\n ('107', ()),\n ('96', ()),\n ('99', ()),\n ('102',\n  (Node('/DH/101/102/103'), Node('/DH/101/102/104'), Node('/DH/101/102/105'))),\n ('103', ()),\n ('104', ()),\n ('105', (Node('/DH/101/102/105/106'),)),\n ('106', ())]\n\n\n\nnn_np_loi = [(node.name, node.parent) for node in LevelOrderIter(root)]\n\n\nall_str_to_node\n\n{'100': Node('/DH/100'),\n '101': Node('/DH/101'),\n '102': Node('/DH/101/102'),\n '103': Node('/DH/101/102/103'),\n '104': Node('/DH/101/102/104'),\n '105': Node('/DH/101/102/105'),\n '106': Node('/DH/101/102/105/106'),\n '107': Node('/DH/107'),\n '95': Node('/DH/95'),\n '96': Node('/DH/95/96'),\n '97': Node('/DH/97'),\n '98': Node('/DH/98'),\n '99': Node('/DH/98/99')}\n\n\n\nfor node, parent in nn_np_loi[::-1][:-1]:\n  print(node)\n  for child in all_str_to_node[node].children:\n    print(child)\n\n106\n105\nNode('/DH/101/102/105/106')\n104\n103\n102\nNode('/DH/101/102/103')\nNode('/DH/101/102/104')\nNode('/DH/101/102/105')\n99\n96\n107\n101\nNode('/DH/101/102')\n100\n98\nNode('/DH/98/99')\n97\n95\nNode('/DH/95/96')\n\n\n\ndh\n\n\n\n\n\n\n\n\nDrill Hole\nUnnamed: 1\nFrom (m)\nTo (m)\nWidth (m)\nGold (g/t)\nRecord\nInterval\nInterval_obj\n\n\n\n\n95\nBR-022\nincluding\n110.0\n116.10\n6.10\n2.62\n95\n(110.0, 116.1]\nInterval(110.000000000000, 116.100000000000)\n\n\n96\nBR-022\nand including\n111.4\n113.10\n1.70\n7.92\n96\n(111.4, 113.1]\nInterval(111.400000000000, 113.100000000000)\n\n\n97\nBR-022\nand\n274.0\n299.00\n25.00\n0.19\n97\n(274.0, 299.0]\nInterval(274.000000000000, 299.000000000000)\n\n\n98\nBR-022\nand\n432.9\n439.00\n6.10\n4.05\n98\n(432.9, 439.0]\nInterval(432.900000000000, 439.000000000000)\n\n\n99\nBR-022\nincluding\n432.9\n435.70\n2.80\n8.18\n99\n(432.9, 435.7]\nInterval(432.900000000000, 435.700000000000)\n\n\n100\nBR-022\nand\n445.0\n452.00\n7.00\n0.41\n100\n(445.0, 452.0]\nInterval(445.000000000000, 452.000000000000)\n\n\n101\nBR-022\nand\n461.6\n512.00\n50.40\n1.78\n101\n(461.6, 512.0]\nInterval(461.600000000000, 512.000000000000)\n\n\n102\nBR-022\nincluding\n471.0\n512.00\n41.00\n2.09\n102\n(471.0, 512.0]\nInterval(471.000000000000, 512.000000000000)\n\n\n103\nBR-022\nand including\n471.0\n478.00\n7.00\n2.37\n103\n(471.0, 478.0]\nInterval(471.000000000000, 478.000000000000)\n\n\n104\nBR-022\nand including\n490.0\n491.00\n1.00\n8.15\n104\n(490.0, 491.0]\nInterval(490.000000000000, 491.000000000000)\n\n\n105\nBR-022\nand including\n505.2\n508.75\n3.55\n14.90\n105\n(505.2, 508.75]\nInterval(505.200000000000, 508.750000000000)\n\n\n106\nBR-022\nand including\n506.2\n506.70\n0.50\n100.48\n106\n(506.2, 506.7]\nInterval(506.200000000000, 506.700000000000)\n\n\n107\nBR-022\nand\n600.0\n620.00\n20.00\n0.52\n107\n(600.0, 620.0]\nInterval(600.000000000000, 620.000000000000)\n\n\n\n\n\n\n\n\ncross_interval = itertools.product(dh.Interval_obj,dh.Interval_obj)\n\nintersect_matrix = np.array([interval[0].intersect(interval[1]) for interval in cross_interval])\n\nintersect_matrix\n\narray([Interval(110.000000000000, 116.100000000000),\n       Interval(111.400000000000, 113.100000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       Interval(111.400000000000, 113.100000000000),\n       Interval(111.400000000000, 113.100000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(),\n       Interval(274.000000000000, 299.000000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(),\n       Interval(432.900000000000, 439.000000000000),\n       Interval(432.900000000000, 435.700000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       Interval(432.900000000000, 435.700000000000),\n       Interval(432.900000000000, 435.700000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(),\n       Interval(445.000000000000, 452.000000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(),\n       Interval(461.600000000000, 512.000000000000),\n       Interval(471.000000000000, 512.000000000000),\n       Interval(471.000000000000, 478.000000000000),\n       Interval(490.000000000000, 491.000000000000),\n       Interval(505.200000000000, 508.750000000000),\n       Interval(506.200000000000, 506.700000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), Interval(471.000000000000, 512.000000000000),\n       Interval(471.000000000000, 512.000000000000),\n       Interval(471.000000000000, 478.000000000000),\n       Interval(490.000000000000, 491.000000000000),\n       Interval(505.200000000000, 508.750000000000),\n       Interval(506.200000000000, 506.700000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), Interval(471.000000000000, 478.000000000000),\n       Interval(471.000000000000, 478.000000000000),\n       Interval(471.000000000000, 478.000000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       Interval(490.000000000000, 491.000000000000),\n       Interval(490.000000000000, 491.000000000000), EmptySet(),\n       Interval(490.000000000000, 491.000000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(),\n       Interval(505.200000000000, 508.750000000000),\n       Interval(505.200000000000, 508.750000000000), EmptySet(),\n       EmptySet(), Interval(505.200000000000, 508.750000000000),\n       Interval(506.200000000000, 506.700000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), Interval(506.200000000000, 506.700000000000),\n       Interval(506.200000000000, 506.700000000000), EmptySet(),\n       EmptySet(), Interval(506.200000000000, 506.700000000000),\n       Interval(506.200000000000, 506.700000000000), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(), EmptySet(), EmptySet(), EmptySet(),\n       EmptySet(), EmptySet(),\n       Interval(600.000000000000, 620.000000000000)], dtype=object)\n\n\n\ndh['grade_len'] = dh['Gold (g/t)'] * dh['Width (m)']\n\n/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"Entry point for launching an IPython kernel.\n\n\n\ndh\n\n\n\n\n\n\n\n\nDrill Hole\nUnnamed: 1\nFrom (m)\nTo (m)\nWidth (m)\nGold (g/t)\nRecord\nInterval\nInterval_obj\ngrade_len\n\n\n\n\n95\nBR-022\nincluding\n110.0\n116.10\n6.10\n2.62\n95\n(110.0, 116.1]\nInterval(110.000000000000, 116.100000000000)\n15.982\n\n\n96\nBR-022\nand including\n111.4\n113.10\n1.70\n7.92\n96\n(111.4, 113.1]\nInterval(111.400000000000, 113.100000000000)\n13.464\n\n\n97\nBR-022\nand\n274.0\n299.00\n25.00\n0.19\n97\n(274.0, 299.0]\nInterval(274.000000000000, 299.000000000000)\n4.750\n\n\n98\nBR-022\nand\n432.9\n439.00\n6.10\n4.05\n98\n(432.9, 439.0]\nInterval(432.900000000000, 439.000000000000)\n24.705\n\n\n99\nBR-022\nincluding\n432.9\n435.70\n2.80\n8.18\n99\n(432.9, 435.7]\nInterval(432.900000000000, 435.700000000000)\n22.904\n\n\n100\nBR-022\nand\n445.0\n452.00\n7.00\n0.41\n100\n(445.0, 452.0]\nInterval(445.000000000000, 452.000000000000)\n2.870\n\n\n101\nBR-022\nand\n461.6\n512.00\n50.40\n1.78\n101\n(461.6, 512.0]\nInterval(461.600000000000, 512.000000000000)\n89.712\n\n\n102\nBR-022\nincluding\n471.0\n512.00\n41.00\n2.09\n102\n(471.0, 512.0]\nInterval(471.000000000000, 512.000000000000)\n85.690\n\n\n103\nBR-022\nand including\n471.0\n478.00\n7.00\n2.37\n103\n(471.0, 478.0]\nInterval(471.000000000000, 478.000000000000)\n16.590\n\n\n104\nBR-022\nand including\n490.0\n491.00\n1.00\n8.15\n104\n(490.0, 491.0]\nInterval(490.000000000000, 491.000000000000)\n8.150\n\n\n105\nBR-022\nand including\n505.2\n508.75\n3.55\n14.90\n105\n(505.2, 508.75]\nInterval(505.200000000000, 508.750000000000)\n52.895\n\n\n106\nBR-022\nand including\n506.2\n506.70\n0.50\n100.48\n106\n(506.2, 506.7]\nInterval(506.200000000000, 506.700000000000)\n50.240\n\n\n107\nBR-022\nand\n600.0\n620.00\n20.00\n0.52\n107\n(600.0, 620.0]\nInterval(600.000000000000, 620.000000000000)\n10.400\n\n\n\n\n\n\n\n\nfrom sympy import Union\nimport functools\n\nresid_grades, resid_len = {}, {} \n\nfor node in levelord_nodes[1:]:\n\n  parent_interval = dh[dh['Record'] == float(node[0])]\n  child_names = [child.name for child in node[1]]\n  child_intervals = [dh[dh['Record'] == float(child)] for child in child_names]\n\n  new_interval_obj = parent_interval.Interval_obj.values[0] - Union([child.Interval_obj.values[0] for child in child_intervals])\n\n  l_child_int = Union([intv['Interval_obj'].values[0] for intv in child_intervals])._measure\n\n  lg_child_int = [dh.loc[int(child_name)]['grade_len'] for child_name in child_names]\n\n  lg_total_int = parent_interval.grade_len.values[0]\n\n  residual_grade = (lg_total_int - sum(lg_child_int)) / (new_interval_obj._measure)\n\n  resid_grades[node[0]] = residual_grade\n  resid_len[node[0]] = new_interval_obj\n\n  print(\"Interval:\")\n  print(node[0])\n\n  print(\"Length x Grade:\")\n  print(lg_total_int - sum(lg_child_int))\n\n  print(\"Residual Grade:\")\n  print(residual_grade)\n\nInterval:\n95\nLength x Grade:\n2.517999999999999\nResidual Grade:\n0.572272727272726\nInterval:\n97\nLength x Grade:\n4.75\nResidual Grade:\n0.190000000000000\nInterval:\n98\nLength x Grade:\n1.801000000000002\nResidual Grade:\n0.545757575757574\nInterval:\n100\nLength x Grade:\n2.8699999999999997\nResidual Grade:\n0.410000000000000\nInterval:\n101\nLength x Grade:\n4.022000000000006\nResidual Grade:\n0.427872340425534\nInterval:\n107\nLength x Grade:\n10.4\nResidual Grade:\n0.520000000000000\nInterval:\n96\nLength x Grade:\n13.464\nResidual Grade:\n7.92000000000005\nInterval:\n99\nLength x Grade:\n22.903999999999996\nResidual Grade:\n8.17999999999997\nInterval:\n102\nLength x Grade:\n8.055000000000007\nResidual Grade:\n0.273514431239389\nInterval:\n103\nLength x Grade:\n16.59\nResidual Grade:\n2.37000000000000\nInterval:\n104\nLength x Grade:\n8.15\nResidual Grade:\n8.15000000000000\nInterval:\n105\nLength x Grade:\n2.654999999999994\nResidual Grade:\n0.870491803278683\nInterval:\n106\nLength x Grade:\n50.24\nResidual Grade:\n100.480000000000\n\n\nCheck these solutions: 95 should be easy to validate\nincludes 96\n95 extends from 110.00 m to 116.10 m (l = 6.10 m)\n96 is the interval from 111.40 to 113.10 m (l = 1.70 m)\nLarger interval\n(6.10 m)(2.62 g/t) = (1.7 m)(7.92 g/t) + (4.4 m)(x g/t)\nRearranging terms, we get:\n\\(x = \\frac{(6.10 m)(2.62 g/t) - (1.7 m)(7.92 g/t)}{ 4.4 m }\\)\nSo the residual grade is 0.5723 g/t, which matches the value found above!\n\n((6.10 * 2.62) - (1.7)*(7.92)) / (4.4)\n\n0.5722727272727269\n\n\n\nresid_len\n\n{'100': Interval(445.000000000000, 452.000000000000),\n '101': Interval.Ropen(461.600000000000, 471.000000000000),\n '102': Union(Interval.open(478.000000000000, 490.000000000000), Interval.open(491.000000000000, 505.200000000000), Interval.Lopen(508.750000000000, 512.000000000000)),\n '103': Interval(471.000000000000, 478.000000000000),\n '104': Interval(490.000000000000, 491.000000000000),\n '105': Union(Interval.Ropen(505.200000000000, 506.200000000000), Interval.Lopen(506.700000000000, 508.750000000000)),\n '106': Interval(506.200000000000, 506.700000000000),\n '107': Interval(600.000000000000, 620.000000000000),\n '95': Union(Interval.Ropen(110.000000000000, 111.400000000000), Interval.Lopen(113.100000000000, 116.100000000000)),\n '96': Interval(111.400000000000, 113.100000000000),\n '97': Interval(274.000000000000, 299.000000000000),\n '98': Interval.Lopen(435.700000000000, 439.000000000000),\n '99': Interval(432.900000000000, 435.700000000000)}\n\n\n\nInterval(445.000000000000, 452.000000000000).end\n\n452.000000000000\n\n\n\ndh['Record'].astype(str).map(resid_len)\ndh['Record'].astype(str).map(resid_grades)\n\n95     0.572272727272726\n96      7.92000000000005\n97     0.190000000000000\n98     0.545757575757574\n99      8.17999999999997\n100    0.410000000000000\n101    0.427872340425534\n102    0.273514431239389\n103     2.37000000000000\n104     8.15000000000000\n105    0.870491803278683\n106     100.480000000000\n107    0.520000000000000\nName: Record, dtype: object\n\n\nTODO: Need to split up the non-contiguous segments so that you can actually plot them\nIdea: use .args attribute Problem: This is defined for Interval objects as well and it gets us something we don’t want\n\ndh['Record'].astype(str).map(resid_len)\n\n95     Union(Interval.Ropen(110.000000000000, 111.400...\n96          Interval(111.400000000000, 113.100000000000)\n97          Interval(274.000000000000, 299.000000000000)\n98     Interval.Lopen(435.700000000000, 439.000000000...\n99          Interval(432.900000000000, 435.700000000000)\n100         Interval(445.000000000000, 452.000000000000)\n101    Interval.Ropen(461.600000000000, 471.000000000...\n102    Union(Interval.open(478.000000000000, 490.0000...\n103         Interval(471.000000000000, 478.000000000000)\n104         Interval(490.000000000000, 491.000000000000)\n105    Union(Interval.Ropen(505.200000000000, 506.200...\n106         Interval(506.200000000000, 506.700000000000)\n107         Interval(600.000000000000, 620.000000000000)\nName: Record, dtype: object\n\n\n\n\n\ndef args_extended(interval):\n  if type(interval) == Union:\n    return interval.args\n  else:\n    return interval\n\nremaining_interval_grade = pd.DataFrame({'interval' : dh['Record'].astype(str).map(resid_len), 'grade' : dh['Record'].astype(str).map(resid_grades)})\n\nremaining_interval_grade['split_intervals'] = remaining_interval_grade.interval.apply(args_extended)\n\nrig_exploded = remaining_interval_grade.explode('split_intervals')\n\n\nrig_exploded\n\n\n\n\n\n\n\n\ninterval\ngrade\nsplit_intervals\n\n\n\n\n95\nUnion(Interval.Ropen(110.000000000000, 111.400...\n0.572272727272726\nInterval.Ropen(110.000000000000, 111.400000000...\n\n\n95\nUnion(Interval.Ropen(110.000000000000, 111.400...\n0.572272727272726\nInterval.Lopen(113.100000000000, 116.100000000...\n\n\n96\nInterval(111.400000000000, 113.100000000000)\n7.92000000000005\nInterval(111.400000000000, 113.100000000000)\n\n\n97\nInterval(274.000000000000, 299.000000000000)\n0.190000000000000\nInterval(274.000000000000, 299.000000000000)\n\n\n98\nInterval.Lopen(435.700000000000, 439.000000000...\n0.545757575757574\nInterval.Lopen(435.700000000000, 439.000000000...\n\n\n99\nInterval(432.900000000000, 435.700000000000)\n8.17999999999997\nInterval(432.900000000000, 435.700000000000)\n\n\n100\nInterval(445.000000000000, 452.000000000000)\n0.410000000000000\nInterval(445.000000000000, 452.000000000000)\n\n\n101\nInterval.Ropen(461.600000000000, 471.000000000...\n0.427872340425534\nInterval.Ropen(461.600000000000, 471.000000000...\n\n\n102\nUnion(Interval.open(478.000000000000, 490.0000...\n0.273514431239389\nInterval.open(478.000000000000, 490.000000000000)\n\n\n102\nUnion(Interval.open(478.000000000000, 490.0000...\n0.273514431239389\nInterval.open(491.000000000000, 505.200000000000)\n\n\n102\nUnion(Interval.open(478.000000000000, 490.0000...\n0.273514431239389\nInterval.Lopen(508.750000000000, 512.000000000...\n\n\n103\nInterval(471.000000000000, 478.000000000000)\n2.37000000000000\nInterval(471.000000000000, 478.000000000000)\n\n\n104\nInterval(490.000000000000, 491.000000000000)\n8.15000000000000\nInterval(490.000000000000, 491.000000000000)\n\n\n105\nUnion(Interval.Ropen(505.200000000000, 506.200...\n0.870491803278683\nInterval.Ropen(505.200000000000, 506.200000000...\n\n\n105\nUnion(Interval.Ropen(505.200000000000, 506.200...\n0.870491803278683\nInterval.Lopen(506.700000000000, 508.750000000...\n\n\n106\nInterval(506.200000000000, 506.700000000000)\n100.480000000000\nInterval(506.200000000000, 506.700000000000)\n\n\n107\nInterval(600.000000000000, 620.000000000000)\n0.520000000000000\nInterval(600.000000000000, 620.000000000000)\n\n\n\n\n\n\n\n\nrig_exploded['From'] = rig_exploded.split_intervals.apply(lambda x: x.start).astype(float)\nrig_exploded['To'] = rig_exploded.split_intervals.apply(lambda x: x.end).astype(float)\nrig_exploded['Width'] = (rig_exploded.To - rig_exploded.From).astype(float)\n\nrig_exploded['grade'] = rig_exploded.grade.astype(float)\n\nrig_exploded['drillhole'] = 'BR-022'    \n\n\nrig_exploded[['From', 'To', 'grade', 'Width']]\n\n\n\n\n\n\n\n\nFrom\nTo\ngrade\nWidth\n\n\n\n\n95\n110.00\n111.40\n0.572273\n1.40\n\n\n95\n113.10\n116.10\n0.572273\n3.00\n\n\n96\n111.40\n113.10\n7.920000\n1.70\n\n\n97\n274.00\n299.00\n0.190000\n25.00\n\n\n98\n435.70\n439.00\n0.545758\n3.30\n\n\n99\n432.90\n435.70\n8.180000\n2.80\n\n\n100\n445.00\n452.00\n0.410000\n7.00\n\n\n101\n461.60\n471.00\n0.427872\n9.40\n\n\n102\n478.00\n490.00\n0.273514\n12.00\n\n\n102\n491.00\n505.20\n0.273514\n14.20\n\n\n102\n508.75\n512.00\n0.273514\n3.25\n\n\n103\n471.00\n478.00\n2.370000\n7.00\n\n\n104\n490.00\n491.00\n8.150000\n1.00\n\n\n105\n505.20\n506.20\n0.870492\n1.00\n\n\n105\n506.70\n508.75\n0.870492\n2.05\n\n\n106\n506.20\n506.70\n100.480000\n0.50\n\n\n107\n600.00\n620.00\n0.520000\n20.00\n\n\n\n\n\n\n\n\n\ny_axis = alt.Axis(\n    title='Intercept ID',\n    offset=5,\n    ticks=False,\n    domain=False\n)\n\n\nreqd_cols = ['From', 'To', 'grade', 'Width', 'drillhole']\n\n\nalt.Chart(rig_exploded[reqd_cols]).mark_bar().encode(\n    alt.X('From:Q',\n        scale=alt.Scale(zero=False)),\n    x2='To:Q',\n    y=alt.Y('drillhole:N', axis=y_axis),\n    color=alt.Color('grade:Q', scale=alt.Scale(scheme=\"inferno\")),\n    tooltip=[\n        alt.Tooltip('Width:Q', title='Width'),\n        alt.Tooltip('grade:Q', title='Gold Grade')\n    ]\n).properties(width=800, height=100).configure(background='#D9E9F0').interactive()"
  },
  {
    "objectID": "posts/2022-04-03-rotations.html",
    "href": "posts/2022-04-03-rotations.html",
    "title": "Rotations",
    "section": "",
    "text": "Rotations were a major stumbling block for me in my undergrad applied linear algebra class. I’d taken a 300-level math major linear algebra course, and I figured a 400 level applied linear algebra course would be an easy follow-on to fulfill the upper division requirement for the math minor.\nIt was not. It was very hard. It included computational work and some fairly advanced applications. At the time, I did not know how to code so everything was pretty hopelessly abstract\nNow, armed with 6 years of Python experience, I am working my way through Coding the Matrix, which I recommend very highly. It has helped me gain clarity on some of these challenging concepts. There’s also a philosophical angle to it - throughout Dr. Klein articulates the advantages of mathematical learning through coding (emphasis mine):\nI am convinced that students of mathematics would be better served by a “computational thinking” approach to learning math - less memorization and plug-and-chug, pencil-and-paper scribbling and more reading and writing code - I know I certainly am!\nThe first time I covered this material I struggled very badly in many places. I couldn’t grasp the connection between rotations, the complex plane, and roots of unity. That’s simple enough in retrospect, but the material was overwhelming and the connection was totally lost on me at the time. In the defense of the instructors we were running behind, en route to the discrete Fourier transform and we didn’t really have time to stop for straggler.\nNow, with free time, a textbook, and some Python, I am free to build intuition and see these rotations for myself."
  },
  {
    "objectID": "posts/2022-04-03-rotations.html#parrots-the-right-way-up-all-the-way-down",
    "href": "posts/2022-04-03-rotations.html#parrots-the-right-way-up-all-the-way-down",
    "title": "Rotations",
    "section": "Parrots the right way up, all the way down",
    "text": "Parrots the right way up, all the way down\n\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\nurl = \"https://upload.wikimedia.org/wikipedia/commons/f/fa/Grayscale_8bits_palette_sample_image.png\"\n\nfrom urllib.request import urlopen\nfrom PIL import Image\n\nimg = Image.open(urlopen(url))\nimg\n\n\n\n\n\n\n\n\nThis is the best greyscale image I could find on short order (and it definitely helps to have a greyscale image because we are about to binarize it!) It’s also convenient that it is an image with one and only one “correct” orientation - a right way up!\n\nimport numpy as np\n\nimg_array_ = np.asarray(img)\n# binarize! \nimg_array_bin_ = img_array_ &gt; 128\n\n\nimport matplotlib.pyplot as plt\n\n# Unpacking the \"white\" points and scatterplotting\nplt.scatter(*img_array_bin_.nonzero(), s=0.1)\nplt.gcf().set_size_inches((4, 3))    \n\n\n\n\n\n\n\n\nConverting the image to a numpy array did not go seamlessly. This is not the correct orientation.\nConverting to complex coordinates is the first step towards rotating:\n\nx,y = img_array_bin_.nonzero()\n\ncoords_complex_ = x + y*1j\n\nOur rotate method will return the rotated points\n\nrotate_ = lambda x, theta: x*np.exp(1j * theta)\n\nThe rotation angle should be 180 degrees + 90 degrees to flip and then make vertical. This works out to \\(\\frac{3\\pi}{2}\\) radians:\n\ncc = rotate_(coords_complex_, 3*np.pi/2)\n\nplt.scatter(cc.real, cc.imag, s=0.1)\nplt.gcf().set_size_inches((3, 4))    \n\n\n\n\n\n\n\n\nChanging the angle gets us a slightly different orientation:\n\ncc = rotate_(coords_complex_, 8*np.pi/5)\n\nplt.scatter(cc.real, cc.imag, s=0.1)\nplt.gcf().set_size_inches((3, 4))   \n\n\n\n\n\n\n\n\nIt’s a trivial application but it helped me understand the concepts at work here.\nAs Dr. Klein said:\n\nthey can learn through reading, writing, debugging, and using computer programs.\n\nYes we can!"
  },
  {
    "objectID": "posts/11_05_convnet_house_numbers.html",
    "href": "posts/11_05_convnet_house_numbers.html",
    "title": "ConvNet House Numbers",
    "section": "",
    "text": "I recently encountered a problem at work where we could have made use of georeferenced, exterior images of single-family homes.\n\nBasically like this.\nWe immediately thought of Google Streetview and Zillow, but unfortunately both of those platforms have very restrictive use policies. We also considered using data from assessors’ offices, but that’s difficult to do at scale because of the huge variety of ways that these images are stored, to say nothing of the metadata attached to them.\nSo we set out to find a single source of just images, no licensing or metadata to hold us back. Mapillary provides an open alternative of entirely crowdsourced photos shared from dashcams. Much better than the alternatives, but it leaves us with a different challenge: While we know what street we’re on and roughly where we are, we don’t know the camera parameters, so we can’t precisely associate parts of the image with an address. We don’t even have a particularly good idea of which direction the camera is pointed.\nThe natural solution to this problem is to do what humans did before GPS - look at road signs and house numbers! The address is literally painted all over the structure! We just have to segment it and extract it, ideally in an automated way.\nIt’s difficult to imagine a script that we could explicitly program that would handle all the varying fonts, varying camera angles, varying lighting conditions, varying photo qualities that we’ll see in reality. In short, this is a classic use case for deep learning, so we’ll assemble and train a quick convolutional neural network here!"
  },
  {
    "objectID": "posts/11_05_convnet_house_numbers.html#introduction",
    "href": "posts/11_05_convnet_house_numbers.html#introduction",
    "title": "ConvNet House Numbers",
    "section": "",
    "text": "I recently encountered a problem at work where we could have made use of georeferenced, exterior images of single-family homes.\n\nBasically like this.\nWe immediately thought of Google Streetview and Zillow, but unfortunately both of those platforms have very restrictive use policies. We also considered using data from assessors’ offices, but that’s difficult to do at scale because of the huge variety of ways that these images are stored, to say nothing of the metadata attached to them.\nSo we set out to find a single source of just images, no licensing or metadata to hold us back. Mapillary provides an open alternative of entirely crowdsourced photos shared from dashcams. Much better than the alternatives, but it leaves us with a different challenge: While we know what street we’re on and roughly where we are, we don’t know the camera parameters, so we can’t precisely associate parts of the image with an address. We don’t even have a particularly good idea of which direction the camera is pointed.\nThe natural solution to this problem is to do what humans did before GPS - look at road signs and house numbers! The address is literally painted all over the structure! We just have to segment it and extract it, ideally in an automated way.\nIt’s difficult to imagine a script that we could explicitly program that would handle all the varying fonts, varying camera angles, varying lighting conditions, varying photo qualities that we’ll see in reality. In short, this is a classic use case for deep learning, so we’ll assemble and train a quick convolutional neural network here!"
  },
  {
    "objectID": "posts/11_05_convnet_house_numbers.html#the-dataset",
    "href": "posts/11_05_convnet_house_numbers.html#the-dataset",
    "title": "ConvNet House Numbers",
    "section": "The dataset",
    "text": "The dataset\nFor this demonstration we’ll use the The Street View House Numbers (SVHN) Dataset from Stanford. This dataset contains a large number of labeled examples that we can use to train a convnet to label individual digits of a street number.\n\nimport scipy.io\nimport tensorflow as tf\nfrom matplotlib.pyplot import *\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.python.ops.numpy_ops import np_config\nnp_config.enable_numpy_behavior()\n\nmat = scipy.io.loadmat('/content/drive/MyDrive/DATA/train_32x32.mat')\nmat_test = scipy.io.loadmat('/content/drive/MyDrive/DATA/test_32x32.mat')\n\nThis is what the 32x32 pixel digits look like:\n\nax = imshow(mat[\"X\"][:,:,:,3])\n\n\n\n\n\n\n\n\n\nax = imshow(mat[\"X\"][:,:,:,115])\n\n\n\n\n\n\n\n\nAnd the associated labels:\n\nmat[\"y\"][3]\n\narray([3], dtype=uint8)\n\n\n\nmat[\"y\"][115]\n\narray([3], dtype=uint8)\n\n\nEven this toy example includes some of the challenges we listed above (varying fonts, varying camera angles, varying lighting conditions, varying photo qualities) - and others that we didn’t discuss:\n\nCharacter level ground truth in an MNIST-like format. All digits have been resized to a fixed resolution of 32-by-32 pixels. The original character bounding boxes are extended in the appropriate dimension to become square windows, so that resizing them to 32-by-32 pixels does not introduce aspect ratio distortions. Nevertheless this preprocessing introduces some distracting digits to the sides of the digit of interest."
  },
  {
    "objectID": "posts/11_05_convnet_house_numbers.html#preprocessing",
    "href": "posts/11_05_convnet_house_numbers.html#preprocessing",
    "title": "ConvNet House Numbers",
    "section": "Preprocessing",
    "text": "Preprocessing\nWe need to turn our array of RGB images into an array of greyscale images (it’s definitely possible to do deep learning with RGB images, but it’s a little more involved) And doing a little reshaping as well - need it in batch, pixel_x, pixel_y format.\nWe’re using the\n\nx_train = tf.transpose(np.tensordot(mat[\"X\"], [0.299, 0.587, 0.114], axes=([2], [0])), [2, 0, 1])\nx_test = tf.transpose(np.tensordot(mat_test[\"X\"], [0.299, 0.587, 0.114], axes=([2], [0])), [2, 0, 1])\n\nx_train.shape\n\nTensorShape([73257, 32, 32])\n\n\nWe’re using the \\(v_{gray}(r,g,b) = 0.2989 * r + 0.5870 * g + 0.1140 * b\\) formula to convert the (r,g,b) values into a grayscale value.\n\nimshow(x_train[3,:,:])\n\n\n\n\n\n\n\n\nNew! with zero color!\nNow we move on to preparing the labels and further altering the shape of our tensor.\n\nnum_classes = len(np.unique(mat[\"y\"])) + 1\nprint(f\"Num classes: {num_classes}\")\n\ny_train = mat[\"y\"].ravel()\ny_test = mat_test[\"y\"].ravel()\n\ninput_shape = (32, 32, 1)\n\ndef scale_and_expand(x_):\n  return np.expand_dims((x_.astype(\"float32\") / 255), -1)\n\n\nx_train, x_test = (scale_and_expand(x) for x in (x_train, x_test))\n\nprint(\"x_train shape:\", x_train.shape)\nprint(x_train.shape[0], \"train samples\")\nprint(x_test.shape[0], \"test samples\")\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nNum classes: 11\nx_train shape: (73257, 32, 32, 1)\n73257 train samples\n26032 test samples"
  },
  {
    "objectID": "posts/11_05_convnet_house_numbers.html#model-topology",
    "href": "posts/11_05_convnet_house_numbers.html#model-topology",
    "title": "ConvNet House Numbers",
    "section": "Model Topology",
    "text": "Model Topology\nThis is a simple ConvNet architecture (borrowed from here in the keras docs, credit to François Chollet)\n\nmodel = keras.Sequential(\n    [\n        keras.Input(shape=input_shape),\n        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Flatten(),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation=\"softmax\"),\n    ]\n)\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 30, 30, 32)        320       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n 2D)                                                             \n                                                                 \n flatten (Flatten)           (None, 2304)              0         \n                                                                 \n dropout (Dropout)           (None, 2304)              0         \n                                                                 \n dense (Dense)               (None, 11)                25355     \n                                                                 \n=================================================================\nTotal params: 44,171\nTrainable params: 44,171\nNon-trainable params: 0\n_________________________________________________________________"
  },
  {
    "objectID": "posts/11_05_convnet_house_numbers.html#parameters-and-training",
    "href": "posts/11_05_convnet_house_numbers.html#parameters-and-training",
    "title": "ConvNet House Numbers",
    "section": "Parameters and Training",
    "text": "Parameters and Training\n\nbatch_size = 128\nepochs = 15\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n\nEpoch 1/15\n516/516 [==============================] - 101s 195ms/step - loss: 1.5204 - accuracy: 0.5037 - val_loss: 0.7716 - val_accuracy: 0.7935\nEpoch 2/15\n516/516 [==============================] - 87s 168ms/step - loss: 0.8102 - accuracy: 0.7637 - val_loss: 0.6294 - val_accuracy: 0.8354\nEpoch 3/15\n516/516 [==============================] - 77s 150ms/step - loss: 0.7143 - accuracy: 0.7977 - val_loss: 0.5900 - val_accuracy: 0.8460\nEpoch 4/15\n516/516 [==============================] - 89s 173ms/step - loss: 0.6613 - accuracy: 0.8141 - val_loss: 0.5400 - val_accuracy: 0.8598\nEpoch 5/15\n516/516 [==============================] - 88s 170ms/step - loss: 0.6289 - accuracy: 0.8232 - val_loss: 0.5257 - val_accuracy: 0.8597\nEpoch 6/15\n516/516 [==============================] - 83s 161ms/step - loss: 0.6051 - accuracy: 0.8284 - val_loss: 0.5008 - val_accuracy: 0.8677\nEpoch 7/15\n516/516 [==============================] - 80s 155ms/step - loss: 0.5827 - accuracy: 0.8354 - val_loss: 0.4865 - val_accuracy: 0.8657\nEpoch 8/15\n516/516 [==============================] - 79s 152ms/step - loss: 0.5680 - accuracy: 0.8377 - val_loss: 0.4763 - val_accuracy: 0.8647\nEpoch 9/15\n516/516 [==============================] - 79s 152ms/step - loss: 0.5498 - accuracy: 0.8421 - val_loss: 0.4652 - val_accuracy: 0.8743\nEpoch 10/15\n516/516 [==============================] - 78s 151ms/step - loss: 0.5381 - accuracy: 0.8475 - val_loss: 0.4508 - val_accuracy: 0.8721\nEpoch 11/15\n516/516 [==============================] - 82s 158ms/step - loss: 0.5284 - accuracy: 0.8489 - val_loss: 0.4342 - val_accuracy: 0.8793\nEpoch 12/15\n516/516 [==============================] - 78s 152ms/step - loss: 0.5147 - accuracy: 0.8515 - val_loss: 0.4243 - val_accuracy: 0.8804\nEpoch 13/15\n516/516 [==============================] - 78s 151ms/step - loss: 0.5066 - accuracy: 0.8536 - val_loss: 0.4201 - val_accuracy: 0.8795\nEpoch 14/15\n516/516 [==============================] - 78s 152ms/step - loss: 0.4945 - accuracy: 0.8564 - val_loss: 0.4294 - val_accuracy: 0.8855\nEpoch 15/15\n516/516 [==============================] - 78s 151ms/step - loss: 0.4897 - accuracy: 0.8574 - val_loss: 0.4050 - val_accuracy: 0.8833\n\n\n&lt;keras.callbacks.History at 0x7f4416c3fe10&gt;\n\n\nLooking at the train/validation scores in the diagnostic output, I always have to remind myself that dropout means that validation accuracy will often be higher than training accuracy! Otherwise quite happy with the convergence of our model. Let’s see how it does on a held-out test set:\n\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])\n\nTest loss: 0.4587405323982239\nTest accuracy: 0.8741933107376099\n\n\nNot too bad for a first pass!\nSources:\n\nHigher validation accuracy, than training accurracy using Tensorflow and Keras\nCS231n Convolutional Neural Networks for Visual Recognition: Lecture 3, Learning\nThe Street View House Numbers (SVHN) Dataset\nSimple MNIST convnet"
  },
  {
    "objectID": "posts/2021-01-19-birthday-problem-here-and-elsewhere.html",
    "href": "posts/2021-01-19-birthday-problem-here-and-elsewhere.html",
    "title": "The birthday problem here and elsewhere",
    "section": "",
    "text": "The birthday problem is a classic probabilistic puzzle concerning the probability of any two people among \\(n\\) people in a group sharing the same birthday.\n\nIn a group of \\(n\\) people, what is the probability that two of these people share a birthday?\n\nThe intuition around the puzzle is very misleading, so much so that it is sometimes called the birthday paradox, even though there is ultimately no paradox in the problem or its solution."
  },
  {
    "objectID": "posts/2021-01-19-birthday-problem-here-and-elsewhere.html#introduction",
    "href": "posts/2021-01-19-birthday-problem-here-and-elsewhere.html#introduction",
    "title": "The birthday problem here and elsewhere",
    "section": "",
    "text": "The birthday problem is a classic probabilistic puzzle concerning the probability of any two people among \\(n\\) people in a group sharing the same birthday.\n\nIn a group of \\(n\\) people, what is the probability that two of these people share a birthday?\n\nThe intuition around the puzzle is very misleading, so much so that it is sometimes called the birthday paradox, even though there is ultimately no paradox in the problem or its solution."
  },
  {
    "objectID": "posts/2021-01-19-birthday-problem-here-and-elsewhere.html#birthday-problem-here",
    "href": "posts/2021-01-19-birthday-problem-here-and-elsewhere.html#birthday-problem-here",
    "title": "The birthday problem here and elsewhere",
    "section": "Birthday Problem Here",
    "text": "Birthday Problem Here\nIn fact, given some reasonable assumptions, there is a simple closed form expression for the probability:\n\\(p(n)=\\frac{365 !}{365^{n}(365-n) !}\\)\nunder the assumption that all \\(n\\) birthdays are drawn uniformly at random from 365 possible dates.\nHowever, this model leads to some very surprising results when we consider the probabilities of finding shared birthdays in groups of different size \\(n\\).\nWe can ask more interesting questions: for instance, what group size \\(n\\) would result in a 50% chance of a shared birthday within the group? As an upper bound on \\(n\\), a party of size \\(n=366\\) must have two individuals that share the same birthday, for \\(p=1.0\\), since there are only 365 unique birthdays to be assigned (by the famous pigeonhole principle). The answer for \\(p=0.5\\), it turns out, is only \\(n=23\\).\nThere is &gt;50% probability that among a group of 23 people, 2 share a birthday!\nThis is such a surprising conclusion that the whole problem is sometimes called a paradox!\nFortunately, verifying this result can be done with simulation just as well as with analytical methods. The code below assembles groups individual by individual, stopping when two individuals share a birthday. This is detected when a list of day numbers differs in size from a set of day numbers, because a set discards duplicates while a list does not:\n\nimport random\n\ndef birthday_sim(n_days=365):\n  dates = []\n  while len(set(dates)) == len(dates):\n    dates.append(random.randint(1,n_days + 1))\n  return len(dates)\n\nThis code can easily run 100,000 simulations:\n\nsims_100k = [birthday_sim() for x in range(100000)]\n\nThe distribution is right-skewed, which we’d expect with a distribution centered at near 20 but taking values up to 365. The peak near 20 confirms the conclusion above that a value in this area corresponds to the 50th percentile, or the approximation of \\(p=0.5\\) in this sample.\n\nimport matplotlib.pyplot as plt\n\nhist100k = plt.hist(sims_100k, bins = 89)\n\n\n\n\n\n\n\n\nThe median represents the 50th percentile/2nd quantile/50% probability and sits exactly at 23:\n\nimport statistics\n\nstatistics.median(sims_100k)\n\n23.0\n\n\nThe CDF gives a nice view of the problem: now we can read off the probabilities and the corresponding group size \\(n\\). Here we find a few more surprising conclusions: we hit 80% probability of having a shared birthday just under \\(n=40\\), and a 90% probability just over \\(n=40\\).\n\nhist100k = plt.hist(sims_100k, bins = 89, cumulative=True, density=True)\n\n\n\n\n\n\n\n\nIn group sizes over \\(n=60\\) finding a shared birthday should be a near certainty. The table puts the probability at 99.4% where \\(n=60\\).\nThis phenomenon is likely so unintuitive partially because the question is misunderstood. “Find any two people with the same birthday” can so easily be misunderstood as “Select one person, and find another person with the same birthday”, or “find one person in the group that shares MY birthday”. Neither of these is the correct interpretation. There is no need to match a specific person, or a specific date. These interpretations lack the combinatorial characteristic that makes these probabilities so high for low \\(n\\); there are just so many different ways of combining \\(n\\) people into groups of two!"
  },
  {
    "objectID": "posts/2021-01-19-birthday-problem-here-and-elsewhere.html#birthday-problem-elsewhere",
    "href": "posts/2021-01-19-birthday-problem-here-and-elsewhere.html#birthday-problem-elsewhere",
    "title": "The birthday problem here and elsewhere",
    "section": "Birthday Problem Elsewhere",
    "text": "Birthday Problem Elsewhere\nAnother advantage of the simulation method is flexibility. If we imagine we’re meeting groups of \\(n\\) individuals born on a different planet with a different year length, there is no need to re-derive the equation above because the simulation method adapts seamlessly to the new problem. We simply have to change the n_days parameter.\nFor example, a single year lasts 88 earth days on Mercury (though a Mercury day lasts 2 Mercury years!):\n\nmercury_sims_100k = [birthday_sim(88) for x in range(100000)]\n\nhist100k = plt.hist(mercury_sims_100k, bins = 39, cumulative=True, density=True)\n\n\n\n\n\n\n\n\n\nstatistics.median(mercury_sims_100k)\n\n12.0\n\n\nSo the corresponding value for 50% probability on Mercury? \\(n=12\\).\nClearly, the simulation technique is well suited to this problem - in fact, this class of problems - and might even be considered an improvement on the available analytical methods!"
  },
  {
    "objectID": "posts/2022-11-04-data-mining-the-water-table.html",
    "href": "posts/2022-11-04-data-mining-the-water-table.html",
    "title": "Data Mining the Water Table",
    "section": "",
    "text": "This is a competition from DrivenData that I’ve wanted to enter for some time - it’s a geospatial problem, and it’s related to water, and it’s related to development - this is a win-win-win from my perspective.\nOf course, the first attempt is never actually going to be about winning, especially in an established competition like this one. Here I’ll just try to figure out what data is available to us and what API the competition requires (what is the input to our pipeline? what does the output look like?) Sometimes I’ll even use randomly generated mock predictions to validate the API - but in this case, I’ll actually put together a simple baseline model.\nFirst things first, what’s the objective here?\nSo we have some working wells, and some broken wells, and it’s our job to figure out which is which.\nNext, some imports to make and some files to open:\n!pip install geopandas &gt; /dev/null\n!pip install folium matplotlib mapclassify &gt; /dev/null\n!pip install contextily &gt; /dev/null\nimport pandas as pd\nimport geopandas as gpd\ntrain_labels = pd.read_csv(\"/content/drive/MyDrive/data_mining_water_table/0bf8bc6e-30d0-4c50-956a-603fc693d966.csv\")\ntrain_features = pd.read_csv(\"/content/drive/MyDrive/data_mining_water_table/4910797b-ee55-40a7-8668-10efd5c1b960.csv\")\ntest_features = pd.read_csv(\"/content/drive/MyDrive/data_mining_water_table/702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv\")"
  },
  {
    "objectID": "posts/2022-11-04-data-mining-the-water-table.html#labels",
    "href": "posts/2022-11-04-data-mining-the-water-table.html#labels",
    "title": "Data Mining the Water Table",
    "section": "Labels",
    "text": "Labels\nLooking at the label values:\n\ntrain_labels.status_group.value_counts()\n\nfunctional                 32259\nnon functional             22824\nfunctional needs repair     4317\nName: status_group, dtype: int64\n\n\nSo this is a multi-class classification problem! I’ll leave that be for the time being, but I think there may be a way around that particular hitch.\nThe class balance looks alright between functional and non functional, but functional needs repair is super underrepresented.\nOnly one more column to check here - it may say it’s a primary key but I’ve been burned too many times:\n\ntrain_labels.id.is_unique\n\nTrue"
  },
  {
    "objectID": "posts/2022-11-04-data-mining-the-water-table.html#features",
    "href": "posts/2022-11-04-data-mining-the-water-table.html#features",
    "title": "Data Mining the Water Table",
    "section": "Features",
    "text": "Features\nAt first glance I prefer to look at the .info() method output on the dataframe. I find it is a happy medium between looking at column names and diving right into the output of df.describe(). Datatypes and variable names tell us a lot about the features and what they could represent!\n\ntrain_features.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 59400 entries, 0 to 59399\nData columns (total 40 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   id                     59400 non-null  int64  \n 1   amount_tsh             59400 non-null  float64\n 2   date_recorded          59400 non-null  object \n 3   funder                 55765 non-null  object \n 4   gps_height             59400 non-null  int64  \n 5   installer              55745 non-null  object \n 6   longitude              59400 non-null  float64\n 7   latitude               59400 non-null  float64\n 8   wpt_name               59400 non-null  object \n 9   num_private            59400 non-null  int64  \n 10  basin                  59400 non-null  object \n 11  subvillage             59029 non-null  object \n 12  region                 59400 non-null  object \n 13  region_code            59400 non-null  int64  \n 14  district_code          59400 non-null  int64  \n 15  lga                    59400 non-null  object \n 16  ward                   59400 non-null  object \n 17  population             59400 non-null  int64  \n 18  public_meeting         56066 non-null  object \n 19  recorded_by            59400 non-null  object \n 20  scheme_management      55523 non-null  object \n 21  scheme_name            31234 non-null  object \n 22  permit                 56344 non-null  object \n 23  construction_year      59400 non-null  int64  \n 24  extraction_type        59400 non-null  object \n 25  extraction_type_group  59400 non-null  object \n 26  extraction_type_class  59400 non-null  object \n 27  management             59400 non-null  object \n 28  management_group       59400 non-null  object \n 29  payment                59400 non-null  object \n 30  payment_type           59400 non-null  object \n 31  water_quality          59400 non-null  object \n 32  quality_group          59400 non-null  object \n 33  quantity               59400 non-null  object \n 34  quantity_group         59400 non-null  object \n 35  source                 59400 non-null  object \n 36  source_type            59400 non-null  object \n 37  source_class           59400 non-null  object \n 38  waterpoint_type        59400 non-null  object \n 39  waterpoint_type_group  59400 non-null  object \ndtypes: float64(3), int64(7), object(30)\nmemory usage: 18.1+ MB\n\n\nWe also get a preview of missingness. In a welcome departure from meticulously curated datasets that often show up in ML competitions, this dataset has missing data. This is a frequent problem in real ML problems and handling it gracefully is important in application.\nWhat sticks out at me:\n\nWe have latitude and longitude here, so we are working with geospatial data as expected (yay!)\nThey are already floats, so we won’t need to deal with strings or WKB or anything\nNone of them are missing!\n\nWhat will probably require more feature engineering work:\n\ndate_recorded - if we have datetimes, we will experience at least one datetime related error (sorry, that’s just the rule!) And the year and/or month will probably be more useful than some high-cardinality year-month-day combination\n\nSpeaking of cardinality - we will need to check on these object columns! We might need to process some of them to prune\nAnd finally - some of these look like spatial subdivisions. Those require special care. I’ll address that later. Fortunately, few of those seem to be missing."
  },
  {
    "objectID": "posts/2022-11-04-data-mining-the-water-table.html#maps",
    "href": "posts/2022-11-04-data-mining-the-water-table.html#maps",
    "title": "Data Mining the Water Table",
    "section": "Maps",
    "text": "Maps\nSo we’ve confirmed that the coordinates exist - what’s our next step?\n\nWhat is absolutely positively the FIRST thing you should do with geospatial data? MAP IT!\n\nThe floating point latitude and longitude will take a little massaging to get into the right datatype for , but it’s not anything geopandas can’t handle:\n\ntrain_feat_gdf = gpd.GeoDataFrame(train_features, \n                                  geometry=gpd.points_from_xy(\n                                      train_features.longitude,\n                                      train_features.latitude)\n                                  ).set_crs(epsg=4326)\n\nThis dataframe is big, so we should probably subsample first and restrict the values we display.\n\nimport contextily as cx\n\nax = train_feat_gdf.merge(train_labels)[[\"geometry\", \"status_group\"]].sample(frac=0.1).to_crs(epsg=3857).plot(figsize=(20, 12),column=\"status_group\", legend=True)\n\n# ax = df_wm.plot(figsize=(10, 10), alpha=0.5, edgecolor='k')\ncx.add_basemap(ax)\n\n\n\n\n\n\n\n\nThe wells are pretty clustered, presumably around human settlements. The sparse areas are reserves and national parks, so we wouldn’t expect a large or permanent human presence in that area. That can all be established from just the label locations, not their color.\nFrom the class labels it is pretty clear that the non-functional wells are clustered in proximity to one another. This shouldn’t be terribly surprising: we can imagine a number of different variables that vary spatially and affect the functioning of the wells (like the depth to water table, or geology, or topography, or a shared maintenance crew etc.)\nThis spatial correlation shows up in many different applications and there are many methods to help us make good spatial predictions. This will probably be important in this particular competition.\nGiven that we can see the spatial correlation visually, it makes sense to exploit it immediately. This can serve as a good baseline method.\nWithin the universe of spatial models there are still many to choose from. The simplest is probably a k nearest neighbors approach - where the class is determined by a vote of the nearest k labeled examples in feature space (in this case, \\((x,y)\\) space).\nkNN is most appropriate for “interpolation” type problems, where the unlabeled data is interspersed with the labeled data and there are plenty of close neighbors to choose from. This is a common feature of spatial algorithms that use weighted combinations of surrounding values. I verified this earlier but didn’t want to include the plot here to avoid bloating the notebook with maps.\nSo we can move right on to preprocessing, parameter selection, and training:\n\nxy_ = train_features.merge(train_labels)\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score, cross_validate\n\n\nX = xy_[[\"longitude\", \"latitude\"]]\ny = xy_[[\"status_group\"]]\n\n\npipe = Pipeline(\n    [\n        (\"s_scaler\", StandardScaler()),\n        (\"knn\", KNeighborsClassifier()),\n    ]\n)\n\nExplicitly setting the value of k to 5 neighbors:\n\n\npipe.set_params(knn__n_neighbors = 5)\n\nscores = cross_validate(pipe, X, y.values.ravel(), cv=5, scoring=[\"roc_auc_ovo\", \"balanced_accuracy\"])\nscores\n\n{'fit_time': array([0.1089015 , 0.08685112, 0.15483212, 0.22506523, 0.21545577]),\n 'score_time': array([0.60315633, 0.88687468, 1.12632608, 1.07287598, 0.4798944 ]),\n 'test_roc_auc_ovo': array([0.75283679, 0.74980639, 0.75823468, 0.74190514, 0.76256156]),\n 'test_balanced_accuracy': array([0.54690659, 0.53857098, 0.54173178, 0.54211186, 0.54501897])}\n\n\n\npipe.set_params(knn__n_neighbors = 7)\n\nscores = cross_validate(pipe, X, y.values.ravel(), cv=5, scoring=[\"roc_auc_ovo\", \"balanced_accuracy\"])\nscores\n\n{'fit_time': array([0.16143775, 0.08543348, 0.09065247, 0.0830009 , 0.08364654]),\n 'score_time': array([0.52614832, 0.47105384, 0.52411175, 0.51431751, 0.57632303]),\n 'test_roc_auc_ovo': array([0.76035781, 0.77105027, 0.76698738, 0.74877248, 0.77267746]),\n 'test_balanced_accuracy': array([0.52641583, 0.52729711, 0.52632049, 0.5237053 , 0.52658191])}\n\n\nAnd now to 15:\n\npipe.set_params(knn__n_neighbors = 15)\n\nscores = cross_validate(pipe, X, y.values.ravel(), cv=5, scoring=[\"roc_auc_ovo\", \"balanced_accuracy\"])\nscores\n\n{'fit_time': array([0.08686829, 0.08921075, 0.08237839, 0.08246255, 0.15993857]),\n 'score_time': array([0.54348087, 0.54373407, 0.54726887, 0.95049524, 0.70698071]),\n 'test_roc_auc_ovo': array([0.77307768, 0.77400969, 0.76989494, 0.76127998, 0.77105859]),\n 'test_balanced_accuracy': array([0.51086299, 0.51277977, 0.5155871 , 0.51080323, 0.51121926])}\n\n\nThe ROC score improved! However the balanced accuracy did not.\nkNN methods are known to struggle with imbalanced classes, and we are likely seeing the minority class get erased from the predictions as the neighborhood size increases and the local majority vote looks more and more like the global majority (which doesn’t include many instances of the minority class, by definition)\nThe competition uses a global accuracy measurement that doesn’t specifically penalize errors on the minority class, so we can simply optimize for AUC and improve our overall classification accuracy.\n\npipe.set_params(knn__n_neighbors = 20)\n\nscores = cross_validate(pipe, X, y.values.ravel(), cv=5, scoring=[\"roc_auc_ovo\", \"balanced_accuracy\"])\nscores\n\n{'fit_time': array([0.22514343, 0.08336806, 0.08348846, 0.08687043, 0.23844385]),\n 'score_time': array([0.80263448, 0.56800866, 0.54393411, 0.59026051, 1.0776782 ]),\n 'test_roc_auc_ovo': array([0.76983834, 0.7714384 , 0.76485649, 0.75795266, 0.76685553]),\n 'test_balanced_accuracy': array([0.49622907, 0.49721959, 0.50113215, 0.50283384, 0.49887269])}\n\n\n\npipe.set_params(knn__n_neighbors = 30)\n\nscores = cross_validate(pipe, X, y.values.ravel(), cv=5, scoring=[\"roc_auc_ovo\", \"balanced_accuracy\"])\nscores\n\n{'fit_time': array([0.0867455 , 0.08558059, 0.08764887, 0.09245777, 0.08821964]),\n 'score_time': array([0.60613155, 0.75048637, 0.71204829, 0.83812332, 0.60349774]),\n 'test_roc_auc_ovo': array([0.76066061, 0.76423555, 0.75789102, 0.75412041, 0.75974524]),\n 'test_balanced_accuracy': array([0.47806289, 0.48623149, 0.48762984, 0.48972768, 0.48137897])}\n\n\nSo based on these experiments (really, just a manual hyperparameter optimization routine) the optimal value for k is around 15. So we can train and predict using k=15:\n\n# neigh = KNeighborsClassifier(n_neighbors=15)\n\npipe.set_params(knn__n_neighbors = 15)\n\npipe = pipe.fit(X, y.values.ravel())\n\nXtest = test_features[[\"longitude\", \"latitude\"]]\ntest_features[\"status_group\"] = pipe.predict(Xtest)\n\n\ntest_features[[\"id\", \"status_group\"]].to_csv(\n    \"/content/drive/MyDrive/data_mining_water_table/nshea3_submission_110422_2.csv\",\n     index=False)\n\nAt the time, that puts me at number 4933 out of 5400 submissions with an accuracy of 0.6737 - which is calculated as \\(\\frac{1}{N} \\sum_{1}^{N} I(y_i = \\hat{y}_i)\\)\nThis result isn’t super impressive but it shows that there is indeed spatial correlation in this problem that can be exploited."
  },
  {
    "objectID": "posts/2022-11-04-data-mining-the-water-table.html#next-steps",
    "href": "posts/2022-11-04-data-mining-the-water-table.html#next-steps",
    "title": "Data Mining the Water Table",
    "section": "Next steps",
    "text": "Next steps\nAs I mentioned before: I don’t like working with multiclass classification problems unless it is completely necessary. Multiclass problems and multiclass models and multiclass model diagnostics+interpretations are more difficult to understand and more difficult to explain. In this case, non functional and functional needs repair could be subsets of needs repair - so we can bundle them into that variable and perform a first stage of binary classification on needs repair versus not needing repair. Or we could bundle first the other way around - we’ll try both and use whatever works best.\nWe’ll address that in combination with the following:\n\nExperiment with AutoML\nAddress the missing data problem\nRigorously model spatial dependence"
  },
  {
    "objectID": "posts/2020-11-20-simple-betting-strategies-python.html",
    "href": "posts/2020-11-20-simple-betting-strategies-python.html",
    "title": "Simple Betting Strategies in Python",
    "section": "",
    "text": "A student of probability is practically guaranteed to see games of chance during their studies - coin flipping, card drawing, dice games. This usually seems contrived, when in fact games of chance have been central to the development of probability and statistics. Unfortunately, the games studied are often simplified and betting strategies are constrained.\nThere are good reasons for this: analysis of betting strategies requires an understanding of simulation, which might be too advanced for an intro probability course. But at a certain point it’s useful to analyze simple toy strategies and discover their extreme properties. The martingale system is one such strategy: a simple betting that recommends doubling the value bet with each loss.\nIt is not a new strategy, as we can gather from this 18th century account:\n\nCasanova’s diary, Venice, 1754: Playing the martingale, and doubling my stakes continuously, I won every day during the remainder of the carnival. [..] I congratulated myself upon having increased the treasure of my dear mistress [..]\n\nThen, several days later:\n\nI still played on the martingale, but with such bad luck that I was soon left without a sequin. As I shared my property with my mistress, [..] at her request I sold all her diamonds, losing what I got for them; she had now only five hundred sequins by her. There was no more talk of her escaping from the convent, for we had nothing to live on!\n\nSource: Betting systems: how not to lose your money gambling (G. Berkolaiko)\nCasanova’s account is important as it reveals some of the shortcomings of the strategy: the strategy does not consider the remaining bankroll, so the gambler with finite resources exposes themselves to the risk of ruin. In slightly more technical terms martingale betting strategy on a fair (50-50, even odds) game is a sure thing only with infinite capital to absorb long losing streaks with exponentially increasing losses. An unfair (subfair) game is a loss no matter the betting strategy (there is theoretical work to support this). Subfair games include most casino games (poker excepted - the casino will never let you pick their pocket but picking another punter’s pocket is a matter of supreme indifference).\nRoulette is one such subfair casino game. Roulette distinguished not only by its but its utility as a litmus for gamblers. Roulette strategies reveal bad gamblers (who favor magical thinking and lucky numbers), smart gamblers (who strictly use bold play on red or black), and smarter gamblers (to be found in the poker room or at a hedge fund)."
  },
  {
    "objectID": "posts/2020-11-20-simple-betting-strategies-python.html#doubling-down",
    "href": "posts/2020-11-20-simple-betting-strategies-python.html#doubling-down",
    "title": "Simple Betting Strategies in Python",
    "section": "Doubling Down",
    "text": "Doubling Down\nNow we’ve established what happens in a single round of roulette, we need to ask how to make generalizations about the game: what exactly can we control in this game?\n\nType of bets\nValue of bets\n\nFor simplification sake, we’re considering only one type of bet (though the\nThis is where the martingale strategy comes in:\nAfter every win: Reset the wager \\(w\\) to some base wager \\(n\\)\nAfter every loss: Double the wager to \\(2w\\)\nSo after some number of losses (let’s say \\(k\\) losses), the next wager will be \\(n*2^{k}\\)\nNow we have a (not) new, enticingly simple strategy, the first order of business is to determine the performance of this strategy in our game.\nWith some sloppy hardcoding, our simulation code is the follwing:\n\ndef sim1k(win_prob, n=1000):\n\n    # This will be a list of True/False boolean spin outcomes \n    wl_seq = [get_spin_result(win_prob) for sim in range(n)]\n\n    # Use these to collect our winnings, and set our win amount and bet size\n    winnings = []\n    win = 0\n    bet  = 1\n   \n    # Progress through win loss outcomes in wl_seq\n    for wl in wl_seq:\n        # Cut off when total winnings reach $80\n        if win &lt; 80: \n            # Resetting bet to 1 in case of win\n            if wl == True:\n                win = win + bet\n                bet = 1\n            # Doubling in case of loss\n            else:\n                win = win - bet\n                bet = bet * 2\n        winnings.append(win)\n    return winnings\n\n\nwin_prob = (18./38.)\n\ntentrials = [sim1k(win_prob) for x in range(10)]\n\nfor trial in tentrials:\n  plt.plot(range(1000), trial)\n\n\n\n\n\n\n\n\n\nktrials = [sim1k(win_prob) for x in range(1000)] \n#print(sum(np.any(np.array(ktrials) == 80., axis=1)) / len(np.any(np.array(ktrials) == 80., axis=1)))\nktmean = np.mean(ktrials, axis = 0)\nktsd = np.std(ktrials, axis = 0)\nfor arr in (ktmean, ktmean + ktsd, ktmean - ktsd):\n    plt.plot(range(1000), arr)\n\n\n\n\n\n\n\n\n\nktmed = np.median(ktrials, axis = 0)\nfor arr in (ktmed, ktmed + ktsd, ktmed - ktsd):\n    plt.plot(range(1000), arr)\n\n\n\n\n\n\n\n\nSo what’s the problem there?\nDo not track bankroll - we can make huge losses during the interim\nIn any reasonable casino, there is a limit to bet size\nOften the lower ceiling is the size of our own bank account\n\ndef sim1k_bankroll(win_prob, n=1000):\n    wl_seq = [get_spin_result(win_prob) for sim in range(n)]\n\n    winnings = []\n    win = 0\n    bet  = 1\n   \n    for wl in wl_seq:\n        if (win &lt; 80 and win &gt; -256): \n            if wl == True:\n                win = win + bet\n                bet = 1\n            else:\n                win = win - bet\n                bet = min(bet * 2, 256. + win)\n        winnings.append(win)\n    return winnings\n\n\nktrials_real = [sim1k_bankroll(win_prob) for x in range(1000)]\n\nktmean = np.mean(ktrials_real, axis = 0)\nktsd = np.std(ktrials_real, axis = 0)\nktmed = np.median(ktrials_real, axis = 0)\nplt.figure()\nplt.xlim((0,300))\nplt.ylim((-256,100))\nfor arr in (ktmean, ktmean + ktsd, ktmean - ktsd):\n    plt.plot(range(1000), arr)\n\n\n\n\n\n\n\n\n\nfor arr in (ktmed, ktmed + ktsd, ktmed - ktsd):\n    plt.plot(range(1000), arr)"
  },
  {
    "objectID": "posts/2021-10-19-tedium-free-mle.html",
    "href": "posts/2021-10-19-tedium-free-mle.html",
    "title": "Tedium Free MLE",
    "section": "",
    "text": "Maximum likelihood estimation has the dubious honor of being difficult for humans and machines alike (difficult for machines at least in the naïve formulation that doesn’t use log-likelihood).\nMLE is challenging for humans because it requires the multiplication of \\(n\\) likelihood expressions, which is time consuming and error prone - this is the tedium part we’re trying to avoid. Fortunately, computers are very good at repeated multiplication, even repeated symbolic multiplication."
  },
  {
    "objectID": "posts/2021-10-19-tedium-free-mle.html#introduction",
    "href": "posts/2021-10-19-tedium-free-mle.html#introduction",
    "title": "Tedium Free MLE",
    "section": "",
    "text": "Maximum likelihood estimation has the dubious honor of being difficult for humans and machines alike (difficult for machines at least in the naïve formulation that doesn’t use log-likelihood).\nMLE is challenging for humans because it requires the multiplication of \\(n\\) likelihood expressions, which is time consuming and error prone - this is the tedium part we’re trying to avoid. Fortunately, computers are very good at repeated multiplication, even repeated symbolic multiplication."
  },
  {
    "objectID": "posts/2021-10-19-tedium-free-mle.html#problem-formulation-and-example",
    "href": "posts/2021-10-19-tedium-free-mle.html#problem-formulation-and-example",
    "title": "Tedium Free MLE",
    "section": "Problem Formulation and Example",
    "text": "Problem Formulation and Example\nMLE estimates parameters of an assumed probability distribution, given data \\(x_i\\) observed independently from the same distribution. If that distribution has probability function \\(f(\\cdot)\\), then the likelihood of \\(x_i\\) is \\(f(x_i)\\).\nAs the \\(x_i\\)s are independent, the likelihood of all \\(x_i\\)s will be the product of their individual likelihoods. In mathematical notation, the product will be:\n\\[\\prod_{i=1}^{n} f(x_i)\\]\nProbability functions (mass functions or density functions) like our \\(f(\\cdot)\\) typically have parameters. For instance, the Gaussian distribution has parameters \\(\\mu\\) and \\(\\sigma^2\\), and the Poisson distribution has rate parameter λ. We use MLE to estimate these parameters, so they are the unknowns in the expression and they will appear in each \\(f(x_i)\\) term. We can restate the problem as an equality with the generic parameter \\(\\theta\\):\n\\[L(\\theta) = \\prod_{i=1}^{n} f(x_i)\\]\nThe expression \\(L(\\theta)\\) is the likelihood. In order to find the MLE it is necessary to maximize this function, or find the value of \\(\\theta\\) for which \\(L(\\theta)\\) is as large as possible. This process is probably easier to show than to describe. In particular, we’ll be demonstrating the usefulness of the sympy module in making these symbolic calculations.\n\nExample\nSay we observed values \\([3,1,2]\\) generated from a Poisson. What is likelihood function of λ?\nImporting the necessities and setting up some symbols and expressions:\n\nfrom sympy.stats import Poisson, density, E, variance\nfrom sympy import Symbol, simplify\nfrom sympy.abc import x\n\nlambda_ = Symbol(\"lambda\", positive=True)\n\nf = Poisson(\"f\", lambda_)\ndensity(f)(x)\n\n\\(\\displaystyle \\frac{\\lambda^{x} e^{- \\lambda}}{x!}\\)\n\n\nsympy gives us a representation of the Poisson density to work with in the Poisson() object, keeping track of all of the terms internally.\nThe likelihood expression is the product of the probability function evaluated at these three points:\n\nL_ = density(f)(3) * density(f)(1) * density(f)(2)\nL_\n\n\\(\\displaystyle \\frac{\\lambda^{6} e^{- 3 \\lambda}}{12}\\)\n\n\nThat’s our expression for the likelihood \\(L(\\theta)\\) 🙂 In order to maximize the expression, we’ll take the derivative expression and then solve for the value of parameter \\(\\lambda\\) where the derivative expression is equal to 0. This value of \\(\\lambda\\) will maximize the likelihood.\nFinding the derivative using sympy:\n\nfrom sympy import diff\n\ndL_ = diff(L_, lambda_)\ndL_\n\n\\(\\displaystyle - \\frac{\\lambda^{6} e^{- 3 \\lambda}}{4} + \\frac{\\lambda^{5} e^{- 3 \\lambda}}{2}\\)\n\n\nSetting the derivative \\(\\frac{dL}{d\\theta}\\) equal to zero:\n\nfrom sympy import Eq\n\n\ndLeqz = Eq(dL_, 0)\ndLeqz\n\n\\(\\displaystyle - \\frac{\\lambda^{6} e^{- 3 \\lambda}}{4} + \\frac{\\lambda^{5} e^{- 3 \\lambda}}{2} = 0\\)\n\n\nAnd finally, solving the equation for \\(\\lambda\\):\n\nfrom sympy import solve\n\nsolve(dLeqz, lambda_)\n\n[2]\n\n\nAnd that’s our answer!"
  },
  {
    "objectID": "posts/2021-10-19-tedium-free-mle.html#complications",
    "href": "posts/2021-10-19-tedium-free-mle.html#complications",
    "title": "Tedium Free MLE",
    "section": "Complications",
    "text": "Complications\nThere is a slight wrinkle with this approach. It is susceptible to numerical instability, which (luckily) did not affect us in this example. This is how MLE can become difficult for computers too.\nLikelihoods are usually very small numbers and computers simply can’t track numbers that are too small or too large. Multiplying very small numbers together repeatedly makes very VERY small numbers that can sometimes disappear completely. Without getting too distracted by the minutiae of numerical stability or underflow, we can still appreciate some bizarre behavior that results when floats are misused:\n\n6.89 + .1\n\n6.989999999999999\n\n\n\n(0.1)**512\n\n0.0\n\n\nIn the second scenario, we can imagine having 512 data points and finding that the likelihood evaluates to 0.1 (times our parameter) for every single one. Then our product would look like \\(g(\\theta) \\cdot (0.1)^{512}\\). The computer just told us that one of those terms is zero, and we’re left unable to find the parameters for our MLE."
  },
  {
    "objectID": "posts/2021-10-19-tedium-free-mle.html#solution",
    "href": "posts/2021-10-19-tedium-free-mle.html#solution",
    "title": "Tedium Free MLE",
    "section": "Solution",
    "text": "Solution\nWhat do we do instead? Is there any way to make these numbers bigger, without changing the problem or solution? Is there an equivalent problem with bigger numbers?\nAdding a number and multiplying by a number don’t fix the problem - they just add terms to the expression, which ends up zero anyhow. However these functions do have one property that we will need to be sure we are solving an equivalent problem: they preserve the order of the input in the output. We call these functions monotonic.\nThe monotonic functions also include the log function. The log function has some very nice properties, not least of which that it makes our calculations immune to the problems we saw above. Calculating the log likelihood:\n\nfrom sympy import log\n\n_ = simplify(log(L_))\n_\n\n\\(\\displaystyle - 3 \\lambda + 6 \\log{\\left(\\lambda \\right)} - \\log{\\left(12 \\right)}\\)\n\n\nAnd then taking the derivative as before:\n\nd_ = diff(_, lambda_)\nd_\n\nSetting equal to zero:\n\n_ = Eq(_, 0)\n_\n\n\\(\\displaystyle -3 + \\frac{6}{\\lambda} = 0\\)\n\n\nAnd solving:\n\nfrom sympy import solve\n\nsolve(_, lambda_)\n\n[2]\n\n\nThe two solutions agree! Which is necessary, but not sufficient to show these methods are equivalent in general."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nshea3blog",
    "section": "",
    "text": "ConvNet House Numbers\n\n\n\nkeras\n\ncnn\n\ndeeplearning\n\nneuralnets\n\n\n\n\n\n\n\n\n\nnshea3\n\n\n\n\n\n\n\n\n\n\n\n\nData Mining the Water Table\n\n\n\n\n\n\n\n\nNov 4, 2022\n\n\nnshea3\n\n\n\n\n\n\n\n\n\n\n\n\nRotations\n\n\n\n\n\n\n\n\nApr 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nTedium Free MLE\n\n\n\n\n\n\n\n\nOct 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nThe birthday problem here and elsewhere\n\n\n\n\n\n\n\n\nJan 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nThe Mathematics of Drilling Intercepts\n\n\n\n\n\n\n\n\nDec 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Betting Strategies in Python\n\n\n\npython\n\nsimulation\n\nbetting\n\nstrategy\n\nmartingale\n\n\n\nSimple betting strategies in Python\n\n\n\n\n\nNov 20, 2020\n\n\nnshea3\n\n\n\n\n\nNo matching items"
  }
]